{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string, unicodedata\n",
    "import contractions\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import time\n",
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.max_length = 600000000\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_esa_text():\n",
    "    \"\"\"Extract ESA Text from text files\"\"\"\n",
    "    esa_text = []\n",
    "    files = os.listdir(\"G:/Post Construction/ESA_text\")\n",
    "    for file in files:\n",
    "        with codecs.open(\"G:/Post Construction/ESA_text/\" + file,'r', encoding='utf-8-sig') as corpus:\n",
    "            input_str = corpus.read()\n",
    "            esa_text.append(input_str)\n",
    "    return esa_text\n",
    "\n",
    "def get_pcmr_text():\n",
    "    \"\"\"Extract PCMR Text from text files\"\"\"\n",
    "    pcmr_text = []\n",
    "    files = os.listdir(\"G:/Post Construction/PDF_text\")\n",
    "    for file in files:\n",
    "        with codecs.open(\"G:/Post Construction/PDF_text/\" + file,'r', encoding='utf-8-sig') as corpus:\n",
    "            input_str = corpus.read()\n",
    "            pcmr_text.append(input_str)\n",
    "    return pcmr_text\n",
    "\n",
    "def combine_text():\n",
    "    \"\"\"combine text string from ESA and PCMR text\"\"\"\n",
    "    esa_corpus = get_esa_text()\n",
    "    pcmr_corpus = get_pcmr_text()\n",
    "    corpus = esa_corpus + pcmr_corpus\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise Removal and Text Corpus Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from text string i.e. converting accented characters/letters\"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def to_lowercase(text):\n",
    "    \"\"\"Convert all characters to lowercase from text string\"\"\"\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"convert word in the text string to its root form\"\"\"\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits = False):\n",
    "    \"\"\"Removing non-alphanumeric characters and symbols or even ocasionally numeric characters\"\"\"\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stop words from text string\"\"\"\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')     \n",
    "    return ' '.join(word for word in text.split() if word not in stopword_list)\n",
    "\n",
    "def prohibitedWords(text):\n",
    "    \"\"\"The list of words to be removed from the SQL database issues table to avoid capturing false positives\"\"\"\n",
    "    text = text.split()\n",
    "    prohibitedWordList = ['issue', 'become', 'therefore', 'monitor', 'compare', 'observe', 'risk', 'construct', 'part', 'conduct', 'focus', 'prior', 'manage', 'consider', 'moderate', 'condition', 'potential', 'action', 'reassess', 'row', 'impact', 'control', 'management', 'good', 'unique', 'introduce', 'list', 'potentially', 'low', 'establish', 'legislation']\n",
    "    resultwords  = [word for word in text if word not in prohibitedWordList]\n",
    "    text = ' '.join(resultwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "compaction contouring_ subsidence_ admixing_ contouring_ crowning_ loss agricultural capability equivalent land capability achieve wetlandss scalping\n"
    }
   ],
   "source": [
    "text = 'compaction ( ) contouring_1 subsidence_1 admixing_1 contouring_1 crowning_1 1 . loss of agricultural capability 2016 â€“ equivalent land capability have be achieve .6 . < s > wetlands</s > scalping'\n",
    "pattern = '|'.join(['<s>', '</s>',])\n",
    "text = text.replace(pattern, '')\n",
    "text = remove_non_ascii(text)\n",
    "text = to_lowercase(text)\n",
    "pattern = '|'.join(['<s>', '</s>',])\n",
    "text = text.replace(pattern, '')\n",
    "text = remove_special_characters(text, remove_digits= True)\n",
    "#text = lemmatize_text(text)\n",
    "text = remove_stopwords(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if re.search(r'\\b' + 'subsidence' + r'\\b', text):\n",
    "    print('present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'soil': ['alleviate',\n  'pellet inject',\n  'productivity lower',\n  'topography class',\n  'topsoil',\n  'bulk density',\n  'testing',\n  'plant vigor',\n  'notation',\n  'sampling',\n  'nearly level',\n  'soil',\n  'discing',\n  'admix',\n  'description apprev',\n  'crop growth',\n  'phase',\n  'severe',\n  'stunted crop',\n  'alleviate compaction',\n  'productivity',\n  'resistance',\n  'cultivate varying',\n  'admixing',\n  'texture',\n  'depth',\n  'differentiate subsoil',\n  'warrant alleviate',\n  'obo',\n  'break level',\n  'shovel resistance',\n  'unit',\n  'rutting',\n  'investigation site',\n  'gentle slope',\n  'minor',\n  'penetration',\n  'unit depth',\n  'altalis township',\n  'reduce',\n  'subsoil',\n  'physical resistance',\n  'organic carbon',\n  'saobo',\n  'para till',\n  'rut',\n  'shags topographic',\n  'reduced crop'],\n 'vegetation': ['compare',\n  'management',\n  'species',\n  'wd woody',\n  'control',\n  'rare plant',\n  'per square',\n  'forbs shrub',\n  'patch',\n  'exhibit excellent',\n  'liverwort',\n  'plant',\n  'row',\n  'sweet clover',\n  'species longer',\n  'good',\n  'threat',\n  'sna exotic',\n  'unique',\n  'downy brome',\n  'appropriate situation',\n  'potentially',\n  'andor aggressive',\n  'growth',\n  'prohibit noxious',\n  'legislation',\n  'spray',\n  'community',\n  'common ragweed',\n  'goldthread coptis',\n  'wild carrot',\n  'weed',\n  'herbaceous',\n  'fruit',\n  'seedle',\n  'desirable specie',\n  'sp mound',\n  'seedling present',\n  'rare vascular',\n  'density distribution',\n  'composition',\n  'vegetation',\n  'false ragweed',\n  'infestation',\n  'narrow workspace',\n  'air shovel',\n  'manage',\n  'consider',\n  'shift ditchline',\n  'nuisance',\n  'monitor',\n  'specie',\n  'type nesting',\n  'treatment',\n  'invasion',\n  'undesirable specie',\n  'particularly trench',\n  'thistle',\n  'btnn btnn',\n  'seed catch',\n  'establishment',\n  'late seral',\n  'low',\n  'mow',\n  'recognize',\n  'uncommon',\n  'absinthe',\n  'ecological community',\n  'occurrence',\n  'observe',\n  'comparable',\n  'debris seedle',\n  'desirable',\n  'cover',\n  'ecological',\n  'seedle planting',\n  'crop performance',\n  'research',\n  'andor esc',\n  'list',\n  'acims',\n  'manual mechanical',\n  'common burdock',\n  'treatment mound',\n  'mature',\n  'noxious prohibit',\n  'issue',\n  'vascular plant',\n  'long term',\n  'throughout',\n  'expect improve',\n  'prevent spread',\n  'invasive',\n  'special conservation',\n  'undesirable',\n  'tkrs',\n  'functional',\n  'enhancement avoid',\n  'colonial',\n  'scentless chamomile',\n  'competitive',\n  'potential threat',\n  'mowed',\n  'establish',\n  'yet establish',\n  'noxious',\n  'moderate',\n  'white cockle',\n  'introduce',\n  'unsuccessful',\n  'golden saxifrage',\n  'migration stop'],\n 'water': ['quality',\n  'body',\n  'saturation',\n  'presence snag',\n  'semi saturate',\n  'cap function',\n  'submergent riparian',\n  'flow',\n  'fully saturate',\n  'surface',\n  'stunted crop',\n  'water',\n  'saturated',\n  'cm mottling',\n  'exhibit semi',\n  'ppm',\n  'groundwater',\n  'ponding',\n  'well aesrd',\n  'exhibit fully',\n  'open',\n  'stunt crop',\n  'submergent',\n  'pouch',\n  'compaction rutting'],\n 'fish': ['amphibian',\n  'downstream',\n  'fish bearing',\n  'fish',\n  'dam pump',\n  'habitat',\n  'sport',\n  'brook stickleback',\n  'beaver dam',\n  'instream',\n  'flow',\n  'restored',\n  'isolation',\n  'overhang',\n  'rear',\n  'stream',\n  'fish bear',\n  'fisheries',\n  'white sucker',\n  'dfo',\n  'capture',\n  'spawning',\n  'aar',\n  'northern pike',\n  'riffle pool',\n  'aquatic'],\n 'wildlife': ['ag er',\n  'trench bedrock',\n  'fish fish',\n  'risk',\n  'habitat',\n  'westland resource',\n  'mea sure',\n  'appr tera',\n  'topsoil salvageproc',\n  'special status',\n  'legend rev',\n  'os aic',\n  'utilization',\n  'potential',\n  'historical resource',\n  'mitig ation',\n  'edure',\n  'mammal',\n  'deer',\n  'heritage traditional',\n  'wildlife',\n  'heritage andtraditional',\n  'draw scale',\n  'status endanger'],\n 'species': ['vascular plant',\n  'species',\n  'bromus',\n  'salix sp',\n  'hesperostipa comata',\n  'plant',\n  'awnless brome',\n  'fern',\n  'poa pratensis',\n  'slender',\n  'sna exotic',\n  'invasive',\n  'neb submissions',\n  'exotic',\n  'development restriction',\n  'common name',\n  'ssp',\n  'specie',\n  'scientific name',\n  'become',\n  'native',\n  'colt',\n  'tansy',\n  'acims',\n  'population',\n  'blue'],\n 'air': ['cleaning',\n  'impact',\n  'respect',\n  'fish fish',\n  'power wash',\n  'air',\n  'entrance',\n  'farm',\n  'risk',\n  'part',\n  'variance minor',\n  'enter',\n  'visitor',\n  'traditional',\n  'travel',\n  'mist',\n  'dust',\n  'kill',\n  'bleach',\n  'therefore',\n  'construct',\n  'prior entry',\n  'follow subsection',\n  'potential',\n  'special conservation',\n  'wash station',\n  'human occupancy',\n  'shovel compress',\n  'shoofly',\n  'historical resource',\n  'warrant alleviate',\n  'heritage resource',\n  'initial secondary',\n  'arrive',\n  'cleaning station',\n  'compress air',\n  'penetration',\n  'density hard',\n  'shovel sweep',\n  'equipment',\n  'proceed',\n  'organic farm',\n  'affect',\n  'operate',\n  'contribution',\n  'station',\n  'esa'],\n 'heritage': ['corporation saskatchewan',\n  'credit',\n  'farm rehabilitation',\n  'fwfw',\n  'craik',\n  'facility eh',\n  'slope extreme',\n  'po box',\n  'archaeological',\n  'conflict',\n  'paleontological',\n  'notation',\n  'variance minor',\n  'conservation',\n  'manitoba dept',\n  'smith',\n  'special status',\n  'shoo fly',\n  'loreburn',\n  'resources',\n  'property',\n  'conservation data',\n  'municipality boundary',\n  'ministry',\n  'permit',\n  'human occupancy',\n  'ig iv',\n  'act clearance',\n  'exist',\n  'location',\n  'important bird',\n  'route mile',\n  'canada goose',\n  'st clair',\n  'footprint avoidance',\n  'mile post',\n  'branch',\n  'heritage',\n  'saskatchewan ministry',\n  'winnipeg manitoba',\n  'road',\n  'discovery',\n  'fw fwtwl',\n  'cultural',\n  'albert',\n  'saskatchewan',\n  'administration',\n  'culture'],\n 'physical': ['datum collection',\n  'ponde water',\n  'control',\n  'rilling',\n  'slump',\n  'daaa',\n  'sediment',\n  'special status',\n  'backfill instability',\n  'microtopography',\n  'cf consistency',\n  'stunted crop',\n  'cattle exclusion',\n  'rill',\n  'wind',\n  'rp riparian',\n  'rutting',\n  'wind water ',\n  'roughness',\n  'ponding',\n  'contouring',\n  'contour',\n  'capability stripping',\n  'rv channel',\n  'cm deep',\n  'daylighte hole',\n  'reassess',\n  'recommendation',\n  'action',\n  'recontour',\n  'debris',\n  'restriction restr',\n  'microtopography describe',\n  'cumulative nolf',\n  'monitor',\n  'trench crown',\n  'diversion berm',\n  'slope',\n  'straw wattle',\n  'subsided',\n  'physical environment',\n  'repeat standard',\n  'instability fill',\n  'seed catch',\n  'excess coarse',\n  'nolf nolf',\n  'etc',\n  'methodology preliminary',\n  'crowning',\n  'course fragment',\n  'stone gravel',\n  'point horizon',\n  'contour restoration',\n  'repair',\n  'fragment',\n  'woody debris',\n  'slumping',\n  'profile rv',\n  'andor esc',\n  'disk',\n  'cho',\n  'quality quantity',\n  'coarse fragment',\n  'rut',\n  'subsidence',\n  'nolf',\n  'issue',\n  'ponde',\n  'landscape',\n  'aio',\n  'slope aspect',\n  'screw anchor',\n  'drainage',\n  'weeds rd',\n  'crop growth',\n  'gully',\n  'erosion',\n  'pc pc',\n  'productivity',\n  'silt fencing',\n  'long wide',\n  'stoniness',\n  'visual evaluation',\n  'debris markers',\n  'minor',\n  'stunt crop',\n  'silt fence'],\n 'wetlands': ['goldbar creek',\n  'facility eh',\n  'trenched',\n  'maintain hydrology',\n  'condition',\n  'willow',\n  'hydrology maintain',\n  'row',\n  'topsoil salvageproc',\n  'instream',\n  'riparian planting',\n  'applicationand planning',\n  'creek',\n  'tributary',\n  'sufficient amount',\n  'alteration',\n  'partially',\n  'pre',\n  'successfully emerge',\n  'flume',\n  'sandy droughty',\n  'proper functional',\n  'time tertiary',\n  'wc wc',\n  'bank approach',\n  'christina river',\n  'fish bearing',\n  'crossing',\n  'upstream',\n  'fish',\n  'dam pump',\n  'wetland',\n  'angusmac creek',\n  'healthy uniform',\n  'epi cromer',\n  'flow',\n  'fully vegetate',\n  'bank',\n  'specify sm',\n  'sufficient break',\n  'gainsborough creek',\n  'channel',\n  'tributary rainbow',\n  'spawning',\n  'unnamed tributary',\n  'facility city',\n  'pocologan river',\n  'marked',\n  'emergent submergent',\n  'native blend',\n  'thimbleberry',\n  'scour',\n  'bed bank',\n  'nonfish bearing',\n  'shrubby fen',\n  'via uniform',\n  'functionality',\n  'dry',\n  'red osi',\n  'treed fen',\n  'contain emergent',\n  'full functionality',\n  'dogwood',\n  'nvc',\n  'slumping',\n  'river',\n  'footprint avoidance',\n  'health ranking',\n  'green alder',\n  'main humber',\n  'bridge',\n  'riprap',\n  'watercourse',\n  'culvert',\n  'cross shoofly',\n  'downstream',\n  'lepreau river',\n  'waterbodie',\n  'grassland',\n  'net loss',\n  'wc',\n  'cross',\n  'function',\n  'pipe installation',\n  'stream',\n  'diversity maintain',\n  'functional',\n  'substrate',\n  'floodplain channel',\n  'sa ska',\n  'deep marsh',\n  'native',\n  'terrain instability',\n  'dennis stream',\n  'bed'],\n 'acoustic': ['impact',\n  'addition',\n  'risk',\n  'part',\n  'variance minor',\n  'habitat',\n  'traditional',\n  'acoustic environment',\n  'noise',\n  'special status',\n  'therefore',\n  'construct',\n  'potential',\n  'special conservation',\n  'human occupancy',\n  'review relevant',\n  'historical resource',\n  'warrant alleviate',\n  'heritage resource',\n  'initial secondary',\n  'amendment specify',\n  'contribution',\n  'instability fill'],\n 'navigation': ['facility eh',\n  'mounding',\n  'trail',\n  'railway',\n  'cross shoofly',\n  'archaeological',\n  'access',\n  'environment',\n  'line sight',\n  'website',\n  'doc',\n  'rollback',\n  'function appropriately',\n  'temporary',\n  'build eh',\n  'mound',\n  'traffic',\n  'navigation',\n  'break',\n  'footprint avoidance',\n  'facility city',\n  'highwayuv road',\n  'road',\n  'contact tenant',\n  'prior',\n  'rd party',\n  'landowner via']}"
     },
     "metadata": {},
     "execution_count": 250
    }
   ],
   "source": [
    "dict_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing it All Together - Building a Text Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_corpus(corpus):\n",
    "    \"\"\"Normalize each document in the corpus\"\"\"\n",
    "    start_time = time.time()\n",
    "    normalized_corpus = []\n",
    "    corpus_size = len(get_pcmr_text())\n",
    "    for doc in corpus:\n",
    "        doc = replace_contractions(doc)\n",
    "        doc = remove_non_ascii(doc)\n",
    "        doc = to_lowercase(doc)\n",
    "        doc = lemmatize_text(doc)\n",
    "        doc = remove_special_characters(doc, remove_digits = True)\n",
    "        doc = remove_stopwords(doc)\n",
    "        normalized_corpus.append(doc)\n",
    "    dur = round(time.time() - start_time)\n",
    "    print(f\"Normalized text from {corpus_size} documents in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    \"\"\"using Gensim's simple text preprocessing to convert document into a list of tokens, ignoring tokens that are too short or too long\"\"\"\n",
    "    start_time = time.time()\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=False))\n",
    "    dur = round(time.time() - start_time)\n",
    "    print(f\"Tokenization and further preprocessing completed in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Normalized text from 584 documents in 2363 seconds (39.38 min or 0.66 hours)\nTokenization and further preprocessing completed in 10 seconds (0.17 min or 0.0 hours)\n"
    }
   ],
   "source": [
    "normalized_tokens = list(sent_to_words(normalize_text_corpus(get_pcmr_text())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total tokens in the final corpus: 3438911\n"
    }
   ],
   "source": [
    "def make_bigrams(normalized_tokens):\n",
    "    \"\"\" create bigrams froms normalized tokens corpus\"\"\"\n",
    "    bigram = gensim.models.Phrases(normalized_tokens, min_count = 18, threshold = 16)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in normalized_tokens]\n",
    "\n",
    "#min_count: ignore all words and bigrams with total collected count lower than this\n",
    "#threshold represents a score threshold for forming the phrases (higher means fewer phrases). A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold\n",
    "\n",
    "normalized_tokens_bigrams = make_bigrams(normalized_tokens)\n",
    "tokens = 0\n",
    "for i in normalized_tokens_bigrams:\n",
    "    tokens += len(i)\n",
    "print(f\"Total tokens in the final corpus: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "###############################################\n",
    "feature_size = 100    # word vector \n",
    "window_context = 10   # context window size i.e. maximum distance between current and predicted word within a sentence\n",
    "min_word_count = 36   # Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them\n",
    "sample = 1e-3         # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "learning_rate = 0.01  # the initial learning rate\n",
    "iterations = 20        # Number of iterations over the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Training Data and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word2vec model creation and training completed in 501 seconds (8.35 min or 0.14 hours)\n"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "w2v_model = Word2Vec(min_count = min_word_count,\n",
    "                     window = window_context,\n",
    "                     size = feature_size,\n",
    "                     sg = 1,\n",
    "                     sample = sample,\n",
    "                     negative = 3,\n",
    "                     iter = iterations,\n",
    "                     workers = 1)\n",
    "w2v_model.build_vocab(normalized_tokens_bigrams)\n",
    "w2v_model.train(normalized_tokens_bigrams, total_examples=w2v_model.corpus_count, epochs = w2v_model.iter)\n",
    "dur = round(time.time() - start_time)\n",
    "print(f\"Word2vec model creation and training completed in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Target Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_lst = ['physical_environment', 'soil', 'vegetation', 'water', 'fish', 'wetland', 'wildlife', 'species', 'air', 'air_quality', 'acoustic_environment', 'heritage', 'heritage_resource', 'access']\n",
    "sub_cat_vec_lst = ['erosion', 'coarse_fragment', 'subsidence', 'compaction', 'watercourse', 'invasive', 'plant', 'weed', 'rare', 'stream', 'riparian']\n",
    "vec_sub_cat = []\n",
    "vec_lst.extend(sub_cat_vec_lst)\n",
    "vec_sub_cat.extend(vec_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_word_dict = {}\n",
    "for root_word in vec_sub_cat:\n",
    "    try:\n",
    "        context_words = w2v_model.wv.most_similar(positive = [root_word],topn = 25)\n",
    "        root_word_dict[root_word] = context_words\n",
    "    except:\n",
    "        root_word_dict[root_word] = 'The word is not in vocabulary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"word2vec_df = pd.DataFrame.from_dict(root_word_dict)\\nword2vec_df.to_csv('word2vec_pcmr_bigrams_sep21.csv', encoding = 'utf-8-sig')\""
     },
     "metadata": {},
     "execution_count": 236
    }
   ],
   "source": [
    "\"\"\"word2vec_df = pd.DataFrame.from_dict(root_word_dict)\n",
    "word2vec_df.to_csv('word2vec_pcmr_bigrams_sep21.csv', encoding = 'utf-8-sig')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_keys(dict, key1, key2):\n",
    "    \"\"\"Merge values of similar context words together\"\"\"\n",
    "    for context_word in dict[key1]:\n",
    "        dict[key2].append(context_word)\n",
    "    del dict[key1]\n",
    "    return dict\n",
    "\n",
    "def append_value(dict, key, value):\n",
    "    \"\"\"Append values in the dictionary\"\"\"\n",
    "    dict[key].append(value)\n",
    "    return dict\n",
    "\n",
    "def append_key_as_value(dict):\n",
    "    \"\"\"Append dictionary key as value\"\"\"\n",
    "    for key in dict:\n",
    "        dict[key].append((key, 1.0))\n",
    "    return dict\n",
    "\n",
    "def remove_underscores_duplicates(dict):\n",
    "    \"\"\"Remove underscores from the dictionary keys and values bigrams followed by removing duplicates from values\"\"\"\n",
    "    dict_final = {}\n",
    "    for key,value in dict.items():\n",
    "        new_key = key.replace('_', ' ')\n",
    "        new_value = [value[0].replace('_', ' ') for value in dict[key]]\n",
    "        dict_final[new_key] = new_value\n",
    "    return {key:list(set(value)) for key, value in dict_final.items()}\n",
    "\n",
    "def replace_keys(dict, old_keys, new_keys):\n",
    "    \"\"\"Replace some of the keys for the purpose of naming consistency in SQL database\"\"\"\n",
    "    for idx, new_key in enumerate(new_keys):\n",
    "        dict[new_key] = dict.pop((old_keys)[idx])\n",
    "    return dict\n",
    "\n",
    "def remove_dictionary_values(dictionary, vec, context_words):\n",
    "    \"\"\"Remove the context words which were incorrectly tagged to VECs in word2vec model\"\"\"\n",
    "    for key, value in dictionary.items():\n",
    "        if key == vec:\n",
    "            for word in context_words:\n",
    "                if word in value:\n",
    "                    value.remove(word)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegetation_context_words = ['wetland', 'subsidence', 'erosion']\n",
    "water_context_words = ['erosion']\n",
    "wildlife_context_words = ['air quality', 'acoustic environment']\n",
    "air_context_words = ['acoustic environment', 'habitat', 'special status', 'wildlife']\n",
    "heritage_context_words = ['acoustic environment', 'air quality']\n",
    "physical_context_words = ['weed', 'admix']\n",
    "wetlands_context_words = ['vegetation']\n",
    "acoustic_context_words = ['air quality', 'course fragment', 'wildlife']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    merge_keys(root_word_dict, 'erosion', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'coarse_fragment', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'subsidence', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'compaction', 'soil')\n",
    "    merge_keys(root_word_dict, 'invasive', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'plant', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'weed', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'rare', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'watercourse', 'wetland')\n",
    "    merge_keys(root_word_dict, 'stream', 'wetland')\n",
    "    merge_keys(root_word_dict, 'riparian', 'wetland')\n",
    "    merge_keys(root_word_dict, 'heritage_resource', 'heritage')\n",
    "    merge_keys(root_word_dict, 'air_quality', 'air')\n",
    "    append_value(root_word_dict, 'physical_environment', ('coarse fragment', 1.0))\n",
    "    append_value(root_word_dict, 'vegetation', ('plant', 1.0))\n",
    "    append_value(root_word_dict, 'vegetation', ('invasive', 1.0))\n",
    "    append_value(root_word_dict, 'wetland', ('watercourse', 1.0))\n",
    "    append_value(root_word_dict, 'access', ('navigation', 1.0))\n",
    "    append_key_as_value(root_word_dict)\n",
    "    dict_final = remove_underscores_duplicates(root_word_dict)\n",
    "    old_keys = ['physical environment','wetland', 'acoustic environment', 'access']\n",
    "    new_keys = ['physical', 'wetlands', 'acoustic', 'navigation']\n",
    "    replace_keys(dict_final, old_keys, new_keys)\n",
    "    remove_dictionary_values(dict_final, 'vegetation', vegetation_context_words)\n",
    "    remove_dictionary_values(dict_final, 'water', water_context_words)\n",
    "    remove_dictionary_values(dict_final, 'wildlife', wildlife_context_words)\n",
    "    remove_dictionary_values(dict_final, 'air', air_context_words)\n",
    "    remove_dictionary_values(dict_final, 'heritage', heritage_context_words)\n",
    "    remove_dictionary_values(dict_final, 'physical', physical_context_words)\n",
    "    remove_dictionary_values(dict_final, 'wetlands', wetlands_context_words)\n",
    "    remove_dictionary_values(dict_final, 'acoustic', acoustic_context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "# Importing environmental variables library that reads from the .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading key-value pairs from the .env file into the OS environment\n",
    "load_dotenv()\n",
    "\n",
    "# Reading the key-value pairs from the OS environment\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASS\")\n",
    "db_hostname = os.getenv(\"DB_HOST\")\n",
    "db_name = os.getenv(\"DB_DATABASE\")\n",
    "\n",
    "# Using those variables in the connection string using \"F\" strings\n",
    "conn_string = f\"mysql+mysqldb://{user}:{password}@{db_hostname}/{db_name}?charset=utf8mb4\"\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM issues;\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df = df.applymap(lambda x: x.strip() if type(x)==str else x) # delete whitespaces\n",
    "    df.vec_pri = df.vec_pri.replace('\\s+', ' ', regex=True) # delete extra space between text strings\n",
    "    df.vec_psec = df.vec_sec.replace('\\s+', ' ', regex=True)\n",
    "    df.vec_pri = df.vec_pri.str.lower()\n",
    "    df.vec_sec = df.vec_sec.str.lower()\n",
    "    df['vec_pri'].fillna('', inplace = True)\n",
    "    df['vec_sec'].fillna('', inplace = True)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(lemmatize_text)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(lemmatize_text)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(prohibitedWords)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(prohibitedWords)\n",
    "    \n",
    "#df.loc[df.vec_pri.str.contains(\"(?i)physical environment\", na = False), 'physical'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_keyword_count = []\n",
    "vec_keywords = []\n",
    "\n",
    "for index, row in enumerate(df.itertuples()):\n",
    "    issue_keyword_count = []\n",
    "    \n",
    "    for key, value in dict_final.items():\n",
    "        counter = 0\n",
    "        keyword = []\n",
    "        for vec in value:\n",
    "            if re.search(r'\\b' + vec + r'\\b', row.vec_pri):\n",
    "                keyword.append(vec)\n",
    "                counter += 1\n",
    "        issue_keyword_count.append(counter)\n",
    "        vec_keywords.append(keyword)\n",
    "        \n",
    "    if sum(issue_keyword_count) == 0:\n",
    "        issue_keyword_count = []\n",
    "        keyword = []\n",
    "        for key, value in dict_final.items():\n",
    "            idx = 0\n",
    "            for vec in value:\n",
    "                if re.search(r'\\b' + vec + r'\\b', row.vec_sec):\n",
    "                    keyword.append(vec)\n",
    "                    idx += 1\n",
    "            issue_keyword_count.append(idx)\n",
    "        vec_keywords.append(keyword)\n",
    "            \n",
    "    vec_keyword_count.append(issue_keyword_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      soil  vegetation  water  fish  wildlife  species  air  heritage  \\\n0        0           0      1     0         0        0    0         0   \n1        0           1      0     0         0        0    0         0   \n2        0           0      0     0         0        0    0         0   \n3        0           3      0     0         0        1    0         0   \n4        0           0      0     0         0        0    0         0   \n...    ...         ...    ...   ...       ...      ...  ...       ...   \n5185     0           0      0     0         0        0    0         0   \n5186     0           0      0     0         0        0    0         0   \n5187     0           1      0     0         0        0    0         0   \n5188     0           0      0     0         0        0    0         0   \n5189     1           0      0     0         0        0    0         0   \n\n      physical  wetlands  acoustic  navigation  \n0            2         0         0           0  \n1            0         0         0           0  \n2            1         0         0           0  \n3            0         0         0           0  \n4            1         0         0           0  \n...        ...       ...       ...         ...  \n5185         0         0         0           0  \n5186         0         0         0           0  \n5187         1         1         0           0  \n5188         0         0         0           0  \n5189         3         0         0           0  \n\n[5190 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>soil</th>\n      <th>vegetation</th>\n      <th>water</th>\n      <th>fish</th>\n      <th>wildlife</th>\n      <th>species</th>\n      <th>air</th>\n      <th>heritage</th>\n      <th>physical</th>\n      <th>wetlands</th>\n      <th>acoustic</th>\n      <th>navigation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>5185</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>5186</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>5187</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>5188</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>5189</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5190 rows × 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 395
    }
   ],
   "source": [
    "# Create the pandas DataFrame  \n",
    "df1 = pd.DataFrame(vec_keyword_count, columns = dict_final.keys()) \n",
    "df1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = pd.concat([df, df1], axis = 1)\n",
    "df2['VECassigned'] = df2[['soil', 'vegetation', 'water', 'fish', 'wildlife', 'species', 'air','heritage', 'physical', 'wetlands', 'acoustic', 'navigation']].idxmax(axis=1)\n",
    "for idx, row in df2.iterrows():\n",
    "    if  df2.loc[idx,'vec_simple'] == \"generic\":\n",
    "        df2.loc[idx,'VECassigned'] = \"generic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          vec                                 filing_manual_text\n0        soil  Describe general soil characteristics and the ...\n1  vegetation  For lands where vegetation may be affected by ...\n2       water  Provide a project-specific water use assessmen...\n3        fish  Identify fish species and their life stages in...\n4    wildlife  Identify wildlife species of ecological, econo...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vec</th>\n      <th>filing_manual_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>soil</td>\n      <td>Describe general soil characteristics and the ...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>vegetation</td>\n      <td>For lands where vegetation may be affected by ...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>water</td>\n      <td>Provide a project-specific water use assessmen...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>fish</td>\n      <td>Identify fish species and their life stages in...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>wildlife</td>\n      <td>Identify wildlife species of ecological, econo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "source": [
    "'''df = pd.read_csv('file.csv', encoding='cp1252')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df = df.applymap(lambda x: x.strip() if type(x)==str else x) # delete whitespaces\n",
    "df.filing_manual_text = df.filing_manual_text.replace('\\s+', ' ', regex=True) # delete extra space between text strings\n",
    "df.filing_manual_text = df.filing_manual_text.str.lower()\n",
    "df['filing_manual_text'] = df['filing_manual_text'].apply(lemmatize_text)\n",
    "df['filing_manual_text'] = df['filing_manual_text'].apply(prohibitedWords)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[4, 1, 2, 0, 0, 0, 1, 2, 6, 2, 1, 2],\n [1, 12, 0, 1, 2, 4, 3, 3, 1, 4, 2, 3],\n [3, 1, 6, 1, 0, 0, 1, 1, 1, 5, 1, 0],\n [0, 3, 2, 4, 1, 2, 2, 0, 0, 8, 2, 1],\n [1, 4, 2, 3, 3, 3, 3, 3, 2, 5, 2, 3],\n [1, 3, 0, 3, 3, 4, 2, 4, 2, 3, 3, 1],\n [0, 2, 1, 0, 0, 1, 2, 2, 1, 1, 0, 1],\n [0, 1, 0, 0, 0, 0, 1, 5, 1, 0, 1, 1],\n [2, 1, 3, 2, 0, 0, 1, 1, 5, 7, 0, 0],\n [0, 4, 2, 2, 1, 0, 1, 1, 1, 3, 1, 1],\n [0, 2, 0, 0, 1, 1, 3, 1, 0, 0, 2, 1],\n [0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 2]]"
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "source": [
    "vec_keyword_count = []\n",
    "vec_keywords = []\n",
    "\n",
    "for index, row in enumerate(df.itertuples()):\n",
    "    issue_keyword_count = []\n",
    "    \n",
    "    for key, value in dict_final.items():\n",
    "        counter = 0\n",
    "        keyword = []\n",
    "        for vec in value:\n",
    "            if re.search(r'\\b' + vec + r'\\b', row.filing_manual_text):\n",
    "                keyword.append(vec)\n",
    "                counter += 1\n",
    "        issue_keyword_count.append(counter)\n",
    "        vec_keywords.append(keyword)\n",
    "            \n",
    "    vec_keyword_count.append(issue_keyword_count)\n",
    "\n",
    "vec_keyword_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualizing the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn = 2)] for search_term in ['physical', 'soil', 'erosion', 'vegetation', 'water', 'fish', 'wetland', 'wildlife', 'specie', 'air']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Create TSNE model and plot it\"\n",
    "    words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "    wvs = w2v_model.wv[words]\n",
    "    tsne_model = TSNE(perplexity = 2, n_components = 2, n_iter = 10000, random_state = 0)\n",
    "    np.set_printoptions(suppress = True)\n",
    "    T = tsne_model.fit_transform(wvs)\n",
    "    labels = words\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.scatter(T[:, 0], T[:, 1], c = 'orange', edgecolors = 'r')\n",
    "    for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "        plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit44ddb9ad7b944aa98606efee99bf806f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}