{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string, unicodedata\n",
    "import contractions\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import time\n",
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.max_length = 600000000\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_esa_text():\n",
    "    \"\"\"Extract ESA Text from text files\"\"\"\n",
    "    esa_text = []\n",
    "    files = os.listdir(\"G:/Post Construction/ESA_text\")\n",
    "    for file in files:\n",
    "        with codecs.open(\"G:/Post Construction/ESA_text/\" + file,'r', encoding='utf-8-sig') as corpus:\n",
    "            input_str = corpus.read()\n",
    "            esa_text.append(input_str)\n",
    "    return esa_text\n",
    "\n",
    "def get_pcmr_text():\n",
    "    \"\"\"Extract PCMR Text from text files\"\"\"\n",
    "    pcmr_text = []\n",
    "    files = os.listdir(\"G:/Post Construction/PDF_text\")\n",
    "    for file in files:\n",
    "        with codecs.open(\"G:/Post Construction/PDF_text/\" + file,'r', encoding='utf-8-sig') as corpus:\n",
    "            input_str = corpus.read()\n",
    "            pcmr_text.append(input_str)\n",
    "    return pcmr_text\n",
    "\n",
    "def combine_text():\n",
    "    \"\"\"combine text string from ESA and PCMR text\"\"\"\n",
    "    esa_corpus = get_esa_text()\n",
    "    pcmr_corpus = get_pcmr_text()\n",
    "    corpus = esa_corpus + pcmr_corpus\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise Removal and Text Corpus Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from text string i.e. converting accented characters/letters\"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_between_delimiters(text):\n",
    "    \"\"\"Remove the text between two delimiters < and >\"\"\"\n",
    "    text = re.sub('<[^>]+>', '', text)\n",
    "    return text\n",
    "\n",
    "def to_lowercase(text):\n",
    "    \"\"\"Convert all characters to lowercase from text string\"\"\"\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"convert word in the text string to its root form\"\"\"\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits = False):\n",
    "    \"\"\"Removing non-alphanumeric characters and symbols or even ocasionally numeric characters\"\"\"\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stop words from text string\"\"\"\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')     \n",
    "    return ' '.join(word for word in text.split() if word not in stopword_list)\n",
    "\n",
    "def prohibitedWords(text):\n",
    "    \"\"\"The list of words to be removed from the SQL database issues table to avoid capturing false positives\"\"\"\n",
    "    text = text.split()\n",
    "    prohibitedWordList = ['issue', 'become', 'therefore', 'monitor', 'compare', 'observe', 'construct', 'part', 'conduct', 'focus', 'prior', 'manage', 'consider', 'moderate', 'condition', 'potential', 'action', 'reassess', 'row', 'impact', 'control', 'management', 'good', 'unique', 'introduce', 'list', 'potentially', 'low', 'establish', 'legislation', 'exist', 'nvc']\n",
    "    resultwords  = [word for word in text if word not in prohibitedWordList]\n",
    "    text = ' '.join(resultwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "compaction _ contour _ subsidence _ admix _ contour _ crown _ loss agricultural capability equivalent land capability achieve wetland scalp resume teteatete would\n"
     ]
    }
   ],
   "source": [
    "text = \"compaction _ ( ) contouring_1 subsidence_1 admixing_1 contouring_1 crowning_1 1 . loss of agricultural capability 2016 â€“ equivalent land capability have be achieve .6 . < s > wetlands</s > scalping résumé and tête-à-tête can't wouldn't\"\n",
    "text = replace_contractions(text)\n",
    "text = remove_non_ascii(text)\n",
    "text = remove_between_delimiters(text)\n",
    "text = to_lowercase(text)\n",
    "text = remove_special_characters(text, remove_digits= True)\n",
    "text = lemmatize_text(text)\n",
    "text = remove_stopwords(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing it All Together - Building a Text Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_corpus(corpus):\n",
    "    \"\"\"Normalize each document in the corpus\"\"\"\n",
    "    start_time = time.time()\n",
    "    normalized_corpus = []\n",
    "    corpus_size = len(get_pcmr_text())\n",
    "    for doc in corpus:\n",
    "        doc = replace_contractions(doc)\n",
    "        doc = remove_non_ascii(doc)\n",
    "        doc = remove_between_delimiters(doc)\n",
    "        doc = to_lowercase(doc)\n",
    "        doc = remove_special_characters(doc, remove_digits = True)\n",
    "        doc = lemmatize_text(doc)\n",
    "        doc = remove_stopwords(doc)\n",
    "        normalized_corpus.append(doc)\n",
    "    dur = round(time.time() - start_time)\n",
    "    print(f\"Normalized text from {corpus_size} documents in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    \"\"\"using Gensim's simple text preprocessing to convert document into a list of tokens, ignoring tokens that are too short or too long\"\"\"\n",
    "    start_time = time.time()\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=False))\n",
    "    dur = round(time.time() - start_time)\n",
    "    print(f\"Tokenization and further preprocessing completed in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normalized text from 584 documents in 1661 seconds (27.68 min or 0.46 hours)\n",
      "Tokenization and further preprocessing completed in 10 seconds (0.17 min or 0.0 hours)\n"
     ]
    }
   ],
   "source": [
    "normalized_tokens = list(sent_to_words(normalize_text_corpus(get_pcmr_text())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total tokens in the final corpus: 3208853\n"
     ]
    }
   ],
   "source": [
    "def make_bigrams(normalized_tokens):\n",
    "    \"\"\" create bigrams froms normalized tokens corpus\"\"\"\n",
    "    bigram = gensim.models.Phrases(normalized_tokens, min_count = 18, threshold = 16)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in normalized_tokens]\n",
    "\n",
    "#min_count: ignore all words and bigrams with total collected count lower than this\n",
    "#threshold represents a score threshold for forming the phrases (higher means fewer phrases). A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold\n",
    "\n",
    "normalized_tokens_bigrams = make_bigrams(normalized_tokens)\n",
    "tokens = 0\n",
    "for i in normalized_tokens_bigrams:\n",
    "    tokens += len(i)\n",
    "print(f\"Total tokens in the final corpus: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "###############################################\n",
    "feature_size = 100    # word vector \n",
    "window_context = 10   # context window size i.e. maximum distance between current and predicted word within a sentence\n",
    "min_word_count = 36   # Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them\n",
    "sample = 1e-3         # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "learning_rate = 0.01  # the initial learning rate\n",
    "iterations = 20        # Number of iterations over the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Training Data and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2vec model creation and training completed in 479 seconds (7.98 min or 0.13 hours)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "w2v_model = Word2Vec(min_count = min_word_count,\n",
    "                     window = window_context,\n",
    "                     size = feature_size,\n",
    "                     sg = 1,\n",
    "                     sample = sample,\n",
    "                     negative = 3,\n",
    "                     iter = iterations,\n",
    "                     workers = 1)\n",
    "w2v_model.build_vocab(normalized_tokens_bigrams)\n",
    "w2v_model.train(normalized_tokens_bigrams, total_examples=w2v_model.corpus_count, epochs = w2v_model.iter)\n",
    "dur = round(time.time() - start_time)\n",
    "print(f\"Word2vec model creation and training completed in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Target Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_lst = ['physical_environment', 'soil', 'vegetation', 'water', 'fish', 'wetland', 'wildlife', 'species', 'air', 'air_quality', 'acoustic_environment', 'heritage', 'heritage_resource', 'access']\n",
    "sub_cat_vec_lst = ['erosion', 'coarse_fragment', 'subsidence', 'compaction', 'watercourse', 'invasive', 'plant', 'weed', 'rare', 'stream', 'riparian']\n",
    "vec_sub_cat = []\n",
    "vec_lst.extend(sub_cat_vec_lst)\n",
    "vec_sub_cat.extend(vec_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_word_dict = {}\n",
    "for root_word in vec_sub_cat:\n",
    "    try:\n",
    "        context_words = w2v_model.wv.most_similar(positive = [root_word],topn = 25)\n",
    "        root_word_dict[root_word] = context_words\n",
    "    except:\n",
    "        root_word_dict[root_word] = 'The word is not in vocabulary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_df = pd.DataFrame.from_dict(root_word_dict)\n",
    "word2vec_df.to_csv('word2vecembeddings.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_keys(dict, key1, key2):\n",
    "    \"\"\"Merge values of similar context words together\"\"\"\n",
    "    for context_word in dict[key1]:\n",
    "        dict[key2].append(context_word)\n",
    "    del dict[key1]\n",
    "    return dict\n",
    "\n",
    "def append_value(dict, key, value):\n",
    "    \"\"\"Append values in the dictionary\"\"\"\n",
    "    dict[key].append(value)\n",
    "    return dict\n",
    "\n",
    "def append_key_as_value(dict):\n",
    "    \"\"\"Append dictionary key as value\"\"\"\n",
    "    for key in dict:\n",
    "        dict[key].append((key, 1.0))\n",
    "    return dict\n",
    "\n",
    "def remove_underscores_duplicates(dict):\n",
    "    \"\"\"Remove underscores from the dictionary keys and values bigrams followed by removing duplicates from values\"\"\"\n",
    "    dict_final = {}\n",
    "    for key,value in dict.items():\n",
    "        new_key = key.replace('_', ' ')\n",
    "        new_value = [value[0].replace('_', ' ') for value in dict[key]]\n",
    "        dict_final[new_key] = new_value\n",
    "    return {key:list(set(value)) for key, value in dict_final.items()}\n",
    "\n",
    "def replace_keys(dict, old_keys, new_keys):\n",
    "    \"\"\"Replace some of the keys for the purpose of naming consistency in SQL database\"\"\"\n",
    "    for idx, new_key in enumerate(new_keys):\n",
    "        dict[new_key] = dict.pop((old_keys)[idx])\n",
    "    return dict\n",
    "\n",
    "def remove_dictionary_values(dictionary, vec, context_words):\n",
    "    \"\"\"Remove the context words which were incorrectly tagged to VECs in word2vec model\"\"\"\n",
    "    for key, value in dictionary.items():\n",
    "        if key == vec:\n",
    "            for word in context_words:\n",
    "                if word in value:\n",
    "                    value.remove(word)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegetation_context_words = ['wetland', 'subsidence', 'erosion', 'cover']\n",
    "water_context_words = ['erosion']\n",
    "wildlife_context_words = ['air quality', 'acoustic environment']\n",
    "air_context_words = ['acoustic environment', 'habitat', 'special status', 'wildlife', 'traditional land']\n",
    "heritage_context_words = ['acoustic environment', 'air quality']\n",
    "physical_context_words = ['weed', 'admix']\n",
    "wetlands_context_words = ['vegetation']\n",
    "acoustic_context_words = ['air quality', 'course fragment', 'wildlife']\n",
    "fish_context_words = ['channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    merge_keys(root_word_dict, 'erosion', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'coarse_fragment', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'subsidence', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'compaction', 'soil')\n",
    "    merge_keys(root_word_dict, 'invasive', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'plant', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'weed', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'rare', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'watercourse', 'wetland')\n",
    "    merge_keys(root_word_dict, 'stream', 'wetland')\n",
    "    merge_keys(root_word_dict, 'riparian', 'wetland')\n",
    "    merge_keys(root_word_dict, 'heritage_resource', 'heritage')\n",
    "    merge_keys(root_word_dict, 'air_quality', 'air')\n",
    "    append_value(root_word_dict, 'physical_environment', ('coarse fragment', 1.0))\n",
    "    append_value(root_word_dict, 'physical_environment', ('crown', 1.0))\n",
    "    append_value(root_word_dict, 'vegetation', ('plant', 1.0))\n",
    "    append_value(root_word_dict, 'air', ('quality', 1.0))\n",
    "    append_value(root_word_dict, 'species', ('specie at risk', 1.0))\n",
    "    append_value(root_word_dict, 'vegetation', ('invasive', 1.0))\n",
    "    append_value(root_word_dict, 'wetland', ('watercourse', 1.0))\n",
    "    append_value(root_word_dict, 'access', ('navigation', 1.0))\n",
    "    append_key_as_value(root_word_dict)\n",
    "    dict_final = remove_underscores_duplicates(root_word_dict)\n",
    "    old_keys = ['physical environment','wetland', 'acoustic environment', 'access']\n",
    "    new_keys = ['physical', 'wetlands', 'acoustic', 'navigation']\n",
    "    replace_keys(dict_final, old_keys, new_keys)\n",
    "    remove_dictionary_values(dict_final, 'vegetation', vegetation_context_words)\n",
    "    remove_dictionary_values(dict_final, 'water', water_context_words)\n",
    "    remove_dictionary_values(dict_final, 'wildlife', wildlife_context_words)\n",
    "    remove_dictionary_values(dict_final, 'air', air_context_words)\n",
    "    remove_dictionary_values(dict_final, 'heritage', heritage_context_words)\n",
    "    remove_dictionary_values(dict_final, 'physical', physical_context_words)\n",
    "    remove_dictionary_values(dict_final, 'wetlands', wetlands_context_words)\n",
    "    remove_dictionary_values(dict_final, 'acoustic', acoustic_context_words)\n",
    "    remove_dictionary_values(dict_final, 'fish', fish_context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "# Importing environmental variables library that reads from the .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading key-value pairs from the .env file into the OS environment\n",
    "load_dotenv()\n",
    "\n",
    "# Reading the key-value pairs from the OS environment\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASS\")\n",
    "db_hostname = os.getenv(\"DB_HOST\")\n",
    "db_name = os.getenv(\"DB_DATABASE\")\n",
    "\n",
    "# Using those variables in the connection string using \"F\" strings\n",
    "conn_string = f\"mysql+mysqldb://{user}:{password}@{db_hostname}/{db_name}?charset=utf8mb4\"\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"SELECT distinct i.tableId, i.rowIndex, i.vec_pri, i.vec_sec, i.status_bin, i.status_txt, i.issue_pri, i.issue_sec, p.company, pc.consultantName, pr.application_title_short FROM issues i LEFT JOIN locations l ON i.tableId = l.tableId and i.rowIndex = l.rowIndex LEFT JOIN tables t ON i.tableId = t.headTable LEFT JOIN pdfs p ON t.pdfName = p.pdfName LEFT JOIN projects pr ON p.application_id = pr.application_id LEFT JOIN pdfsconsultants pc ON p.pdfName = pc.pdfName LEFT JOIN tables_tags tt ON t.tableId = tt.tableId WHERE locFormat = 'DLS' AND i.issue_pri NOT IN ('-', '?', 'ESC') AND CONCAT(i.issue_pri,i.issue_sec) <> '----' AND CONCAT(i.issue_pri,i.issue_sec) <> '' AND status_bin NOT IN ('--', '') AND status_bin IS NOT NULL;\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df1 = df.copy()\n",
    "    df = df.applymap(lambda x: x.strip() if type(x)==str else x) # delete whitespaces\n",
    "    df.vec_pri = df.vec_pri.replace('\\s+', ' ', regex=True) # delete extra space between text strings\n",
    "    df.vec_psec = df.vec_sec.replace('\\s+', ' ', regex=True)\n",
    "    df.vec_pri = df.vec_pri.str.lower()\n",
    "    df.vec_sec = df.vec_sec.str.lower()\n",
    "    df['vec_pri'].fillna('', inplace = True)\n",
    "    df['vec_sec'].fillna('', inplace = True)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(remove_between_delimiters)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(remove_between_delimiters)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(remove_special_characters, remove_digits = True)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(remove_special_characters, remove_digits = True)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(lemmatize_text)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(lemmatize_text)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(prohibitedWords)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(prohibitedWords)\n",
    "#df.loc[df.vec_pri.str.contains(\"(?i)physical environment\", na = False), 'physical'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_keyword_count = []\n",
    "vec_keywords = []\n",
    "\n",
    "for index, row in enumerate(df.itertuples()):\n",
    "    issue_keyword_count = []\n",
    "    \n",
    "    for key, value in dict_final.items():\n",
    "        counter = 0\n",
    "        keyword = []\n",
    "        for vec in value:\n",
    "            if re.search(r'\\b' + vec + r'\\b', row.vec_pri):\n",
    "                keyword.append(vec)\n",
    "                counter += 1\n",
    "        issue_keyword_count.append(counter)\n",
    "        vec_keywords.append(keyword)\n",
    "        \n",
    "    if sum(issue_keyword_count) == 0:\n",
    "        issue_keyword_count = []\n",
    "        keyword = []\n",
    "        for key, value in dict_final.items():\n",
    "            idx = 0\n",
    "            for vec in value:\n",
    "                if re.search(r'\\b' + vec + r'\\b', row.vec_sec):\n",
    "                    keyword.append(vec)\n",
    "                    idx += 1\n",
    "            issue_keyword_count.append(idx)\n",
    "        vec_keywords.append(keyword)\n",
    "            \n",
    "    vec_keyword_count.append(issue_keyword_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      soil  vegetation  water  fish  wildlife  species  air  heritage  \\\n",
       "0        2           1      0     0         0        0    0         0   \n",
       "1        2           1      0     0         0        0    0         0   \n",
       "2        1           2      0     0         0        0    0         0   \n",
       "3        0           2      0     0         0        0    0         0   \n",
       "4        0           2      0     0         0        0    0         0   \n",
       "...    ...         ...    ...   ...       ...      ...  ...       ...   \n",
       "1880     0           0      0     0         0        0    0         0   \n",
       "1881     0           2      0     0         0        0    0         0   \n",
       "1882     0           2      0     0         0        0    0         0   \n",
       "1883     0           0      1     0         0        0    0         0   \n",
       "1884     0           0      0     0         0        0    0         0   \n",
       "\n",
       "      physical  wetlands  acoustic  navigation  \n",
       "0            2         0         0           1  \n",
       "1            2         0         0           1  \n",
       "2            1         0         0           0  \n",
       "3            0         0         0           0  \n",
       "4            0         0         0           0  \n",
       "...        ...       ...       ...         ...  \n",
       "1880         0         1         0           0  \n",
       "1881         0         0         0           0  \n",
       "1882         0         0         0           0  \n",
       "1883         1         0         0           0  \n",
       "1884         1         0         0           0  \n",
       "\n",
       "[1885 rows x 12 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>soil</th>\n      <th>vegetation</th>\n      <th>water</th>\n      <th>fish</th>\n      <th>wildlife</th>\n      <th>species</th>\n      <th>air</th>\n      <th>heritage</th>\n      <th>physical</th>\n      <th>wetlands</th>\n      <th>acoustic</th>\n      <th>navigation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1881</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1882</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1883</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1884</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1885 rows × 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "# Create the pandas DataFrame  \n",
    "df2 = pd.DataFrame(vec_keyword_count, columns = dict_final.keys()) \n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3 = pd.concat([df1, df2], axis = 1)\n",
    "df3['VECassigned'] = df3[['soil', 'vegetation', 'water', 'fish', 'wildlife', 'species', 'air','heritage', 'physical', 'wetlands', 'acoustic', 'navigation']].idxmax(axis=1)\n",
    "# for idx, row in df2.iterrows():\n",
    "#     if  df2.loc[idx,'vec_simple'] == \"generic\":\n",
    "#         df2.loc[idx,'VECassigned'] = \"generic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('complete_issues1.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          vec                                 filing_manual_text\n0        soil  Describe general soil characteristics and the ...\n1  vegetation  For lands where vegetation may be affected by ...\n2       water  Provide a project-specific water use assessmen...\n3        fish  Identify fish species and their life stages in...\n4    wildlife  Identify wildlife species of ecological, econo...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vec</th>\n      <th>filing_manual_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>soil</td>\n      <td>Describe general soil characteristics and the ...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>vegetation</td>\n      <td>For lands where vegetation may be affected by ...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>water</td>\n      <td>Provide a project-specific water use assessmen...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>fish</td>\n      <td>Identify fish species and their life stages in...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>wildlife</td>\n      <td>Identify wildlife species of ecological, econo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "source": [
    "'''df = pd.read_csv('file.csv', encoding='cp1252')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df = df.applymap(lambda x: x.strip() if type(x)==str else x) # delete whitespaces\n",
    "df.filing_manual_text = df.filing_manual_text.replace('\\s+', ' ', regex=True) # delete extra space between text strings\n",
    "df.filing_manual_text = df.filing_manual_text.str.lower()\n",
    "df['filing_manual_text'] = df['filing_manual_text'].apply(lemmatize_text)\n",
    "df['filing_manual_text'] = df['filing_manual_text'].apply(prohibitedWords)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[4, 1, 2, 0, 0, 0, 1, 2, 6, 2, 1, 2],\n [1, 12, 0, 1, 2, 4, 3, 3, 1, 4, 2, 3],\n [3, 1, 6, 1, 0, 0, 1, 1, 1, 5, 1, 0],\n [0, 3, 2, 4, 1, 2, 2, 0, 0, 8, 2, 1],\n [1, 4, 2, 3, 3, 3, 3, 3, 2, 5, 2, 3],\n [1, 3, 0, 3, 3, 4, 2, 4, 2, 3, 3, 1],\n [0, 2, 1, 0, 0, 1, 2, 2, 1, 1, 0, 1],\n [0, 1, 0, 0, 0, 0, 1, 5, 1, 0, 1, 1],\n [2, 1, 3, 2, 0, 0, 1, 1, 5, 7, 0, 0],\n [0, 4, 2, 2, 1, 0, 1, 1, 1, 3, 1, 1],\n [0, 2, 0, 0, 1, 1, 3, 1, 0, 0, 2, 1],\n [0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 2]]"
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "source": [
    "vec_keyword_count = []\n",
    "vec_keywords = []\n",
    "\n",
    "for index, row in enumerate(df.itertuples()):\n",
    "    issue_keyword_count = []\n",
    "    \n",
    "    for key, value in dict_final.items():\n",
    "        counter = 0\n",
    "        keyword = []\n",
    "        for vec in value:\n",
    "            if re.search(r'\\b' + vec + r'\\b', row.filing_manual_text):\n",
    "                keyword.append(vec)\n",
    "                counter += 1\n",
    "        issue_keyword_count.append(counter)\n",
    "        vec_keywords.append(keyword)\n",
    "            \n",
    "    vec_keyword_count.append(issue_keyword_count)\n",
    "\n",
    "vec_keyword_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualizing the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn = 2)] for search_term in ['physical', 'soil', 'erosion', 'vegetation', 'water', 'fish', 'wetland', 'wildlife', 'specie', 'air']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Create TSNE model and plot it\"\n",
    "    words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "    wvs = w2v_model.wv[words]\n",
    "    tsne_model = TSNE(perplexity = 2, n_components = 2, n_iter = 10000, random_state = 0)\n",
    "    np.set_printoptions(suppress = True)\n",
    "    T = tsne_model.fit_transform(wvs)\n",
    "    labels = words\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.scatter(T[:, 0], T[:, 1], c = 'orange', edgecolors = 'r')\n",
    "    for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "        plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'similar_words' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-397-abcef5407b0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtsne_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-396-0100a33ce3d4>\u001b[0m in \u001b[0;36mtsne_plot\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtsne_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"Create TSNE model and plot it\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msimilar_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mwvs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtsne_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperplexity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'similar_words' is not defined"
     ]
    }
   ],
   "source": [
    "tsne_plot(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit44ddb9ad7b944aa98606efee99bf806f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}