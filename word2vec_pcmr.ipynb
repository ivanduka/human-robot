{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string, unicodedata\n",
    "import contractions\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import time\n",
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.max_length = 600000000\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_esa_text():\n",
    "    \"\"\"Extract ESA Text from text files\"\"\"\n",
    "    esa_text = []\n",
    "    files = os.listdir(\"G:/Post Construction/ESA_text\")\n",
    "    for file in files:\n",
    "        with codecs.open(\"G:/Post Construction/ESA_text/\" + file,'r', encoding='utf-8-sig') as corpus:\n",
    "            input_str = corpus.read()\n",
    "            esa_text.append(input_str)\n",
    "    return esa_text\n",
    "\n",
    "def get_pcmr_text():\n",
    "    \"\"\"Extract PCMR Text from text files\"\"\"\n",
    "    pcmr_text = []\n",
    "    files = os.listdir(\"G:/Post Construction/PDF_text\")\n",
    "    for file in files:\n",
    "        with codecs.open(\"G:/Post Construction/PDF_text/\" + file,'r', encoding='utf-8-sig') as corpus:\n",
    "            input_str = corpus.read()\n",
    "            pcmr_text.append(input_str)\n",
    "    return pcmr_text\n",
    "\n",
    "def combine_text():\n",
    "    \"\"\"combine text string from ESA and PCMR text\"\"\"\n",
    "    esa_corpus = get_esa_text()\n",
    "    pcmr_corpus = get_pcmr_text()\n",
    "    corpus = esa_corpus + pcmr_corpus\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise Removal and Text Corpus Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from text string i.e. converting accented characters/letters\"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_between_delimiters(text):\n",
    "    \"\"\"Remove the text between two delimiters < and >\"\"\"\n",
    "    text = re.sub('<[^>]+>', '', text)\n",
    "    return text\n",
    "\n",
    "def to_lowercase(text):\n",
    "    \"\"\"Convert all characters to lowercase from text string\"\"\"\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"convert word in the text string to its root form\"\"\"\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits = False):\n",
    "    \"\"\"Removing non-alphanumeric characters and symbols or even ocasionally numeric characters\"\"\"\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stop words from text string\"\"\"\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')     \n",
    "    return ' '.join(word for word in text.split() if word not in stopword_list)\n",
    "\n",
    "def prohibitedWords(text):\n",
    "    \"\"\"The list of words to be removed from the SQL database issues table to avoid capturing false positives\"\"\"\n",
    "    text = text.split()\n",
    "    prohibitedWordList = ['issue', 'become', 'therefore', 'monitor', 'compare', 'observe', 'construct', 'part', 'conduct', 'focus', 'prior', 'manage', 'consider', 'moderate', 'condition', 'potential', 'action', 'reassess', 'row', 'impact', 'control', 'management', 'good', 'unique', 'introduce', 'list', 'potentially', 'low', 'establish', 'legislation', 'exist', 'nvc']\n",
    "    resultwords  = [word for word in text if word not in prohibitedWordList]\n",
    "    text = ' '.join(resultwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "compaction _ contour _ subsidence _ admix _ contour _ crown _ loss agricultural capability equivalent land capability achieve wetland scalp resume teteatete would\n"
     ]
    }
   ],
   "source": [
    "text = \"compaction _ ( ) contouring_1 subsidence_1 admixing_1 contouring_1 crowning_1 1 . loss of agricultural capability 2016 â€“ equivalent land capability have be achieve .6 . < s > wetlands</s > scalping résumé and tête-à-tête can't wouldn't\"\n",
    "text = replace_contractions(text)\n",
    "text = remove_non_ascii(text)\n",
    "text = remove_between_delimiters(text)\n",
    "text = to_lowercase(text)\n",
    "text = remove_special_characters(text, remove_digits= True)\n",
    "text = lemmatize_text(text)\n",
    "text = remove_stopwords(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing it All Together - Building a Text Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_corpus(corpus):\n",
    "    \"\"\"Normalize each document in the corpus\"\"\"\n",
    "    start_time = time.time()\n",
    "    normalized_corpus = []\n",
    "    corpus_size = len(get_pcmr_text())\n",
    "    for doc in corpus:\n",
    "        doc = replace_contractions(doc)\n",
    "        doc = remove_non_ascii(doc)\n",
    "        doc = remove_between_delimiters(doc)\n",
    "        doc = to_lowercase(doc)\n",
    "        doc = remove_special_characters(doc, remove_digits = True)\n",
    "        doc = lemmatize_text(doc)\n",
    "        doc = remove_stopwords(doc)\n",
    "        normalized_corpus.append(doc)\n",
    "    dur = round(time.time() - start_time)\n",
    "    print(f\"Normalized text from {corpus_size} documents in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    \"\"\"using Gensim's simple text preprocessing to convert document into a list of tokens, ignoring tokens that are too short or too long\"\"\"\n",
    "    start_time = time.time()\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=False))\n",
    "    dur = round(time.time() - start_time)\n",
    "    print(f\"Tokenization and further preprocessing completed in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normalized text from 584 documents in 1673 seconds (27.88 min or 0.46 hours)\n",
      "Tokenization and further preprocessing completed in 10 seconds (0.17 min or 0.0 hours)\n"
     ]
    }
   ],
   "source": [
    "normalized_tokens = list(sent_to_words(normalize_text_corpus(get_pcmr_text())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total tokens in the final corpus: 3208853\n"
     ]
    }
   ],
   "source": [
    "def make_bigrams(normalized_tokens):\n",
    "    \"\"\" create bigrams froms normalized tokens corpus\"\"\"\n",
    "    bigram = gensim.models.Phrases(normalized_tokens, min_count = 18, threshold = 16)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in normalized_tokens]\n",
    "\n",
    "#min_count: ignore all words and bigrams with total collected count lower than this\n",
    "#threshold represents a score threshold for forming the phrases (higher means fewer phrases). A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold\n",
    "\n",
    "normalized_tokens_bigrams = make_bigrams(normalized_tokens)\n",
    "tokens = 0\n",
    "for i in normalized_tokens_bigrams:\n",
    "    tokens += len(i)\n",
    "print(f\"Total tokens in the final corpus: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "###############################################\n",
    "feature_size = 100    # word vector \n",
    "window_context = 10   # context window size i.e. maximum distance between current and predicted word within a sentence\n",
    "min_word_count = 36   # Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them\n",
    "sample = 1e-3         # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "learning_rate = 0.01  # the initial learning rate\n",
    "iterations = 20        # Number of iterations over the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Training Data and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2vec model creation and training completed in 442 seconds (7.37 min or 0.12 hours)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "w2v_model = Word2Vec(min_count = min_word_count,\n",
    "                     window = window_context,\n",
    "                     size = feature_size,\n",
    "                     sg = 1,\n",
    "                     sample = sample,\n",
    "                     negative = 3,\n",
    "                     iter = iterations,\n",
    "                     workers = 1)\n",
    "w2v_model.build_vocab(normalized_tokens_bigrams)\n",
    "w2v_model.train(normalized_tokens_bigrams, total_examples=w2v_model.corpus_count, epochs = w2v_model.iter)\n",
    "dur = round(time.time() - start_time)\n",
    "print(f\"Word2vec model creation and training completed in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Target Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_lst = ['physical_environment', 'soil', 'vegetation', 'water', 'fish', 'wetland', 'wildlife', 'species', 'air', 'air_quality', 'acoustic_environment', 'heritage', 'heritage_resource', 'access']\n",
    "sub_cat_vec_lst = ['erosion', 'coarse_fragment', 'subsidence', 'compaction', 'watercourse', 'invasive', 'plant', 'weed', 'rare', 'stream', 'riparian', 'topsoil']\n",
    "vec_sub_cat = []\n",
    "vec_lst.extend(sub_cat_vec_lst)\n",
    "vec_sub_cat.extend(vec_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_word_dict = {}\n",
    "for root_word in vec_sub_cat:\n",
    "    try:\n",
    "        context_words = w2v_model.wv.most_similar(positive = [root_word],topn = 18)\n",
    "        root_word_dict[root_word] = context_words\n",
    "    except:\n",
    "        root_word_dict[root_word] = 'The word is not in vocabulary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_df = pd.DataFrame.from_dict(root_word_dict)\n",
    "word2vec_df.to_csv('word2vecembeddings.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_keys(dict, key1, key2):\n",
    "    \"\"\"Merge values of similar context words together\"\"\"\n",
    "    for context_word in dict[key1]:\n",
    "        dict[key2].append(context_word)\n",
    "    del dict[key1]\n",
    "    return dict\n",
    "\n",
    "def append_value(dict, key, value):\n",
    "    \"\"\"Append values in the dictionary\"\"\"\n",
    "    dict[key].append(value)\n",
    "    return dict\n",
    "\n",
    "def append_key_as_value(dict):\n",
    "    \"\"\"Append dictionary key as value\"\"\"\n",
    "    for key in dict:\n",
    "        dict[key].append((key, 1.0))\n",
    "    return dict\n",
    "\n",
    "def remove_underscores_duplicates(dict):\n",
    "    \"\"\"Remove underscores from the dictionary keys and values bigrams followed by removing duplicates from values\"\"\"\n",
    "    dict_final = {}\n",
    "    for key,value in dict.items():\n",
    "        new_key = key.replace('_', ' ')\n",
    "        new_value = [value[0].replace('_', ' ') for value in dict[key]]\n",
    "        dict_final[new_key] = new_value\n",
    "    return {key:list(set(value)) for key, value in dict_final.items()}\n",
    "\n",
    "def replace_keys(dict, old_keys, new_keys):\n",
    "    \"\"\"Replace some of the keys for the purpose of naming consistency in SQL database\"\"\"\n",
    "    for idx, new_key in enumerate(new_keys):\n",
    "        dict[new_key] = dict.pop((old_keys)[idx])\n",
    "    return dict\n",
    "\n",
    "def remove_dictionary_values(dictionary, vec, context_words):\n",
    "    \"\"\"Remove the context words which were incorrectly tagged to VECs in word2vec model\"\"\"\n",
    "    for key, value in dictionary.items():\n",
    "        if key == vec:\n",
    "            for word in context_words:\n",
    "                if word in value:\n",
    "                    value.remove(word)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegetation_context_words = ['wetland', 'subsidence', 'erosion', 'specie'] ## excluded cover\n",
    "water_context_words = ['erosion']\n",
    "navigation_context_words = ['landowner', 'revegetation']\n",
    "wildlife_context_words = ['air quality', 'acoustic environment', 'fish fish', 'habitat', 'special status']\n",
    "air_context_words = ['acoustic environment', 'habitat', 'special status', 'wildlife', 'traditional land', 'quality', 'risk', 'farm', 'equipment', 'wash', 'part', 'therefore', 'equipment', 'move']\n",
    "heritage_context_words = ['acoustic environment', 'air quality', 'property', 'special status', 'location']\n",
    "physical_context_words = ['weed', 'admix', 'productivity', 'issue', 'drainage']\n",
    "wetlands_context_words = ['vegetation']\n",
    "acoustic_context_words = ['air quality', 'course fragment', 'wildlife', 'risk', 'habitat', 'special status']\n",
    "fish_context_words = ['channel', 'stream', 'instream']\n",
    "species_context_words = ['species', 'specie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    append_key_as_value(root_word_dict)\n",
    "    merge_keys(root_word_dict, 'erosion', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'coarse_fragment', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'subsidence', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'compaction', 'soil')\n",
    "    merge_keys(root_word_dict, 'topsoil', 'soil')\n",
    "    merge_keys(root_word_dict, 'invasive', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'plant', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'weed', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'rare', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'watercourse', 'wetland')\n",
    "    merge_keys(root_word_dict, 'stream', 'wetland')\n",
    "    merge_keys(root_word_dict, 'riparian', 'wetland')\n",
    "    merge_keys(root_word_dict, 'heritage_resource', 'heritage')\n",
    "    merge_keys(root_word_dict, 'air_quality', 'air')\n",
    "    append_value(root_word_dict, 'species', ('wood frog', 1.0))\n",
    "    append_value(root_word_dict, 'species', ('turtle', 1.0))\n",
    "    append_value(root_word_dict, 'species', ('snake', 1.0))\n",
    "    append_value(root_word_dict, 'fish', ('amphibians', 1.0))\n",
    "    append_value(root_word_dict, 'species', ('western toad', 1.0))\n",
    "    append_value(root_word_dict, 'physical_environment', ('crown', 1.0))\n",
    "    append_value(root_word_dict, 'soil', ('soil and soil productivity', 1.0))\n",
    "    append_value(root_word_dict, 'air', ('quality', 1.0))\n",
    "    append_value(root_word_dict, 'species', ('specie at risk', 1.0))\n",
    "    append_value(root_word_dict, 'access', ('navigation', 1.0))\n",
    "    dict_final = remove_underscores_duplicates(root_word_dict)\n",
    "    old_keys = ['physical environment','wetland', 'acoustic environment', 'access']\n",
    "    new_keys = ['physical', 'wetlands', 'acoustic', 'navigation']\n",
    "    replace_keys(dict_final, old_keys, new_keys)\n",
    "    remove_dictionary_values(dict_final, 'vegetation', vegetation_context_words)\n",
    "    remove_dictionary_values(dict_final, 'water', water_context_words)\n",
    "    remove_dictionary_values(dict_final, 'wildlife', wildlife_context_words)\n",
    "    remove_dictionary_values(dict_final, 'air', air_context_words)\n",
    "    remove_dictionary_values(dict_final, 'heritage', heritage_context_words)\n",
    "    remove_dictionary_values(dict_final, 'physical', physical_context_words)\n",
    "    remove_dictionary_values(dict_final, 'wetlands', wetlands_context_words)\n",
    "    remove_dictionary_values(dict_final, 'acoustic', acoustic_context_words)\n",
    "    remove_dictionary_values(dict_final, 'fish', fish_context_words)\n",
    "    remove_dictionary_values(dict_final, 'navigation', navigation_context_words)\n",
    "    remove_dictionary_values(dict_final, 'species', species_context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_clean_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in dict_final.items() ]))\n",
    "# word2vec_clean_df.to_csv('word2vecembeddings1.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "# Importing environmental variables library that reads from the .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading key-value pairs from the .env file into the OS environment\n",
    "load_dotenv()\n",
    "\n",
    "# Reading the key-value pairs from the OS environment\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASS\")\n",
    "db_hostname = os.getenv(\"DB_HOST\")\n",
    "db_name = os.getenv(\"DB_DATABASE\")\n",
    "\n",
    "# Using those variables in the connection string using \"F\" strings\n",
    "conn_string = f\"mysql+mysqldb://{user}:{password}@{db_hostname}/{db_name}?charset=utf8mb4\"\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"SELECT i.tableId, i.rowIndex, i.vec_pri, i.vec_sec FROM issues i where tableId NOT IN ('02db9f91-572a-44af-9858-4add101353c1','03bfc26a-c6d0-4761-b8f5-47acf2290d02','082134c0-6a4b-425b-a4ae-e79acd7316cb','0d10a967-88d6-42e5-9bd7-309f24022b5f','333e1e53-8897-41fa-acbd-86be8afb31c7','35bd2caf-562c-4d14-a5d6-373f168b4acb','397db969-9996-4d9e-bb05-6df69d0fe4a4','417546c4-dacf-4c12-ae75-4dc4e656e198','491c36c1-82d4-46ae-a684-470915a5659b','60b3993d-7075-4790-8519-ba8193579754','64a7ba33-ceee-4593-87a3-8f08dd46c8f4','67691780-af41-414b-a0c2-aa33a3442cdc','6a2f1370-1cd5-4ebb-a4bd-a1fe9d5a516a','6b437f67-967b-4ef6-bd28-5ac8d39138e4','77cc0b8d-8244-4622-8d9d-a56daf6069e8','8bb683d9-f7ee-4a54-ad3d-dddc61ccdfcf','9476acc2-294a-4cd6-a952-8274aedb645a','a6623233-9c9f-436b-ad11-0987ab3825e7','c04807de-2df1-4d26-9352-70d3cb6cb10b','cb197d7e-3ef6-4ee0-93d1-504c7286b580','f143c6b8-cf77-41c1-88b2-e7c97ba657c1','f2ebd484-4ec2-4481-907d-17334ca4657f','f4db9fc5-3a73-499a-ab1e-ab643530ea99','fdb3d057-943a-4fab-99ac-1f4eed471512','44a33e5f-d99e-48ef-ad56-bbb516ec8796','bfafbfd0-8bb5-4283-8f5e-dd7cbcec480c', '3e9e6cdb-f812-4832-b69c-b8ec0396d585');\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df1 = df.copy()\n",
    "    df = df.applymap(lambda x: x.strip() if type(x)==str else x) # delete whitespaces\n",
    "    df.vec_pri = df.vec_pri.replace('\\s+', ' ', regex=True) # delete extra space between text strings\n",
    "    df.vec_sec = df.vec_sec.replace('\\s+', ' ', regex=True)\n",
    "    df.vec_pri = df.vec_pri.str.lower()\n",
    "    df.vec_sec = df.vec_sec.str.lower()\n",
    "    df['vec_pri'].fillna('', inplace = True)\n",
    "    df['vec_sec'].fillna('', inplace = True)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(remove_between_delimiters)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(remove_between_delimiters)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(remove_special_characters, remove_digits = True)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(remove_special_characters, remove_digits = True)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(lemmatize_text)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(lemmatize_text)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(prohibitedWords)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(prohibitedWords)\n",
    "# #df.loc[df.vec_pri.str.contains(\"(?i)physical environment\", na = False), 'physical'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_keyword_count = []\n",
    "vec_keywords = []\n",
    "\n",
    "for index, row in enumerate(df.itertuples()):\n",
    "    issue_keyword_count = []\n",
    "    \n",
    "    for key, value in dict_final.items():\n",
    "        counter = 0\n",
    "        keyword = []\n",
    "        for vec in value:\n",
    "            if re.search(r'\\b' + vec + r'\\b', row.vec_pri):\n",
    "                keyword.append(vec)\n",
    "                counter += 1\n",
    "        issue_keyword_count.append(counter)\n",
    "        vec_keywords.append(keyword)\n",
    "        \n",
    "    if sum(issue_keyword_count) == 0:\n",
    "        issue_keyword_count = []\n",
    "        keyword = []\n",
    "        for key, value in dict_final.items():\n",
    "            idx = 0\n",
    "            for vec in value:\n",
    "                if re.search(r'\\b' + vec + r'\\b', row.vec_sec):\n",
    "                    keyword.append(vec)\n",
    "                    idx += 1\n",
    "            issue_keyword_count.append(idx)\n",
    "        vec_keywords.append(keyword)\n",
    "            \n",
    "    vec_keyword_count.append(issue_keyword_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pandas DataFrame  \n",
    "df2 = pd.DataFrame(vec_keyword_count, columns = dict_final.keys()) \n",
    "df2['threshold'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.where(df2.gt(df2['threshold'],0), ['soil, ', 'vegetation, ', 'water, ', 'fish, ', 'wildlife, ', 'species, ', 'air, ','heritage, ', 'physical, ', 'wetlands, ', 'acoustic, ', 'navigation, ', ''], '')\n",
    "vecs = pd.Series([''.join(x).strip(', ') for x in s], name = \"VECs\")\n",
    "df3 = vecs.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.concat([df1, df2, df3], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.assign(VECs=df4.VECs.str.split(\", \")).explode('VECs')\n",
    "#df4.assign(Book=df.Book.str.split(\",\")).explode('Book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT tableId, rowIndex, rowCounter, issue_parsed FROM issues_parsed;\"\n",
    "with engine.connect() as conn:\n",
    "    issues_parsed_df = pd.read_sql(query, conn)\n",
    "    issues_parsed_df_copy = issues_parsed_df.copy()\n",
    "    issues_parsed_df = issues_parsed_df.applymap(lambda x: x.strip() if type(x)==str else x) # delete whitespaces\n",
    "    issues_parsed_df.issue_parsed = issues_parsed_df.issue_parsed.replace('\\s+', ' ', regex=True) # delete extra space between text strings\n",
    "    issues_parsed_df.issue_parsed = issues_parsed_df.issue_parsed.str.lower()\n",
    "    issues_parsed_df['issue_parsed'].fillna('', inplace = True)\n",
    "    issues_parsed_df['issue_parsed'] = issues_parsed_df['issue_parsed'].apply(remove_between_delimiters)\n",
    "    issues_parsed_df['issue_parsed'] = issues_parsed_df['issue_parsed'].apply(remove_special_characters, remove_digits = True)\n",
    "    issues_parsed_df['issue_parsed'] = issues_parsed_df['issue_parsed'].apply(lemmatize_text)\n",
    "    issues_parsed_df['issue_parsed'] = issues_parsed_df['issue_parsed'].apply(prohibitedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_keyword_count_ip = []\n",
    "for index, row in enumerate(issues_parsed_df.itertuples()):\n",
    "    issue_keyword_count_ip = []\n",
    "    for key, value in dict_final.items():\n",
    "        counter = 0\n",
    "        for vec in value:\n",
    "            if re.search(r'\\b' + vec + r'\\b', row.issue_parsed):\n",
    "                counter += 1\n",
    "        issue_keyword_count_ip.append(counter)           \n",
    "    vec_keyword_count_ip.append(issue_keyword_count_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pandas DataFrame  \n",
    "vec_count_ip_df = pd.DataFrame(vec_keyword_count_ip, columns = dict_final.keys()) \n",
    "vec_count_ip_df['threshold'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_issue_parsed = np.where(vec_count_ip_df.gt(vec_count_ip_df['threshold'],0), ['soil, ', 'vegetation, ', 'water, ', 'fish, ', 'wildlife, ', 'species, ', 'air, ','heritage, ', 'physical, ', 'wetlands, ', 'acoustic, ', 'navigation, ', ''], '')\n",
    "vecs_issue_parsed = pd.Series([''.join(x).strip(', ') for x in s_issue_parsed], name = \"VECs\")\n",
    "df3_issue_parsed = vecs_issue_parsed.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_issue_parsed = pd.concat([issues_parsed_df_copy, vec_count_ip_df, df3_issue_parsed], axis = 1)\n",
    "df4_issue_parsed = df4_issue_parsed.assign(VECs=df4_issue_parsed.VECs.str.split(\", \")).explode('VECs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    final_df = df4.append(df4_issue_parsed, ignore_index=True, sort=False)\n",
    "    return final_df.where(pd.notnull(final_df), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_vecs_table():\n",
    "    insert_vec_query = 'INSERT INTO word2vec (tableId, rowIndex, rowCounter, word2vec_vec) VALUES (%s, %s, %s, %s);'\n",
    "    data = read_data()\n",
    "    with engine.connect() as conn:\n",
    "        for row in data.itertuples():\n",
    "            conn.execute(insert_vec_query, (row.tableId, row.rowIndex, row.rowCounter, row.VECs))\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "populate_vecs_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM word2vec;\"\n",
    "with engine.connect() as conn:\n",
    "    word2vec_df = pd.read_sql(query, conn)\n",
    "word2vec_df.to_csv('vali1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def populate_word2vec_table():\n",
    "    data = df3.to_dict('records')\n",
    "    insert_query = 'INSERT INTO word2vec (word2vec_vec, tableId, rowIndex) VALUE (%s, %s, %s);'\n",
    "    with engine.connect() as conn:\n",
    "        for item in data:\n",
    "            conn.execute(insert_query, (item['VECassigned'], item['tableId'], item['rowIndex']))\n",
    "    print(\"Done\") ## If it fails to insert all rows, it could be because of foreign key constraint error. Refer this link: https://stackoverflow.com/questions/2965837/insert-statement-conflicted-with-the-foreign-key-constraint-sql-server\n",
    "populate_word2vec_table()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df = pd.read_csv('file.csv', encoding='cp1252')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df = df.applymap(lambda x: x.strip() if type(x)==str else x) # delete whitespaces\n",
    "df.filing_manual_text = df.filing_manual_text.replace('\\s+', ' ', regex=True) # delete extra space between text strings\n",
    "df.filing_manual_text = df.filing_manual_text.str.lower()\n",
    "df['filing_manual_text'] = df['filing_manual_text'].apply(lemmatize_text)\n",
    "df['filing_manual_text'] = df['filing_manual_text'].apply(prohibitedWords)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_keyword_count = []\n",
    "vec_keywords = []\n",
    "\n",
    "for index, row in enumerate(df.itertuples()):\n",
    "    issue_keyword_count = []\n",
    "    \n",
    "    for key, value in dict_final.items():\n",
    "        counter = 0\n",
    "        keyword = []\n",
    "        for vec in value:\n",
    "            if re.search(r'\\b' + vec + r'\\b', row.filing_manual_text):\n",
    "                keyword.append(vec)\n",
    "                counter += 1\n",
    "        issue_keyword_count.append(counter)\n",
    "        vec_keywords.append(keyword)\n",
    "            \n",
    "    vec_keyword_count.append(issue_keyword_count)\n",
    "\n",
    "vec_keyword_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualizing the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn = 2)] for search_term in ['physical', 'soil', 'erosion', 'vegetation', 'water', 'fish', 'wetland', 'wildlife', 'specie', 'air']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Create TSNE model and plot it\"\n",
    "    words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "    wvs = w2v_model.wv[words]\n",
    "    tsne_model = TSNE(perplexity = 2, n_components = 2, n_iter = 10000, random_state = 0)\n",
    "    np.set_printoptions(suppress = True)\n",
    "    T = tsne_model.fit_transform(wvs)\n",
    "    labels = words\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.scatter(T[:, 0], T[:, 1], c = 'orange', edgecolors = 'r')\n",
    "    for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "        plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit44ddb9ad7b944aa98606efee99bf806f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}