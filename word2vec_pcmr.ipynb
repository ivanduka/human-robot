{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string, unicodedata\n",
    "import contractions\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import time\n",
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.max_length = 600000000\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_esa_text():\n",
    "    \"\"\"Extract ESA Text from text files\"\"\"\n",
    "    esa_text = []\n",
    "    files = os.listdir(\"G:/Post Construction/ESA_text\")\n",
    "    for file in files:\n",
    "        with codecs.open(\"G:/Post Construction/ESA_text/\" + file,'r', encoding='utf-8-sig') as corpus:\n",
    "            input_str = corpus.read()\n",
    "            esa_text.append(input_str)\n",
    "    return esa_text\n",
    "\n",
    "def get_pcmr_text():\n",
    "    \"\"\"Extract PCMR Text from text files\"\"\"\n",
    "    pcmr_text = []\n",
    "    files = os.listdir(\"G:/Post Construction/PDF_text\")\n",
    "    for file in files:\n",
    "        with codecs.open(\"G:/Post Construction/PDF_text/\" + file,'r', encoding='utf-8-sig') as corpus:\n",
    "            input_str = corpus.read()\n",
    "            pcmr_text.append(input_str)\n",
    "    return pcmr_text\n",
    "\n",
    "def combine_text():\n",
    "    \"\"\"combine text string from ESA and PCMR text\"\"\"\n",
    "    esa_corpus = get_esa_text()\n",
    "    pcmr_corpus = get_pcmr_text()\n",
    "    corpus = esa_corpus + pcmr_corpus\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise Removal and Text Corpus Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from text string i.e. converting accented characters/letters\"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_between_delimiters(text):\n",
    "    \"\"\"Remove the text between two delimiters < and >\"\"\"\n",
    "    text = re.sub('<[^>]+>', '', text)\n",
    "    return text\n",
    "\n",
    "def to_lowercase(text):\n",
    "    \"\"\"Convert all characters to lowercase from text string\"\"\"\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"convert word in the text string to its root form\"\"\"\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits = False):\n",
    "    \"\"\"Removing non-alphanumeric characters and symbols or even ocasionally numeric characters\"\"\"\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stop words from text string\"\"\"\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')     \n",
    "    return ' '.join(word for word in text.split() if word not in stopword_list)\n",
    "\n",
    "def prohibitedWords(text):\n",
    "    \"\"\"The list of words to be removed from the SQL database issues table to avoid capturing false positives\"\"\"\n",
    "    text = text.split()\n",
    "    prohibitedWordList = ['issue', 'become', 'therefore', 'monitor', 'compare', 'observe', 'construct', 'part', 'conduct', 'focus', 'prior', 'manage', 'consider', 'moderate', 'condition', 'potential', 'action', 'reassess', 'row', 'impact', 'control', 'management', 'good', 'unique', 'introduce', 'list', 'potentially', 'low', 'establish', 'legislation', 'exist', 'nvc']\n",
    "    resultwords  = [word for word in text if word not in prohibitedWordList]\n",
    "    text = ' '.join(resultwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "compaction _ contour _ subsidence _ admix _ contour _ crown _ loss agricultural capability equivalent land capability achieve wetland scalp resume teteatete would\n"
     ]
    }
   ],
   "source": [
    "text = \"compaction _ ( ) contouring_1 subsidence_1 admixing_1 contouring_1 crowning_1 1 . loss of agricultural capability 2016 â€“ equivalent land capability have be achieve .6 . < s > wetlands</s > scalping résumé and tête-à-tête can't wouldn't\"\n",
    "text = replace_contractions(text)\n",
    "text = remove_non_ascii(text)\n",
    "text = remove_between_delimiters(text)\n",
    "text = to_lowercase(text)\n",
    "text = remove_special_characters(text, remove_digits= True)\n",
    "text = lemmatize_text(text)\n",
    "text = remove_stopwords(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing it All Together - Building a Text Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_corpus(corpus):\n",
    "    \"\"\"Normalize each document in the corpus\"\"\"\n",
    "    start_time = time.time()\n",
    "    normalized_corpus = []\n",
    "    corpus_size = len(get_pcmr_text())\n",
    "    for doc in corpus:\n",
    "        doc = replace_contractions(doc)\n",
    "        doc = remove_non_ascii(doc)\n",
    "        doc = remove_between_delimiters(doc)\n",
    "        doc = to_lowercase(doc)\n",
    "        doc = remove_special_characters(doc, remove_digits = True)\n",
    "        doc = lemmatize_text(doc)\n",
    "        doc = remove_stopwords(doc)\n",
    "        normalized_corpus.append(doc)\n",
    "    dur = round(time.time() - start_time)\n",
    "    print(f\"Normalized text from {corpus_size} documents in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    \"\"\"using Gensim's simple text preprocessing to convert document into a list of tokens, ignoring tokens that are too short or too long\"\"\"\n",
    "    start_time = time.time()\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=False))\n",
    "    dur = round(time.time() - start_time)\n",
    "    print(f\"Tokenization and further preprocessing completed in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normalized text from 584 documents in 1774 seconds (29.57 min or 0.49 hours)\n",
      "Tokenization and further preprocessing completed in 9 seconds (0.15 min or 0.0 hours)\n"
     ]
    }
   ],
   "source": [
    "normalized_tokens = list(sent_to_words(normalize_text_corpus(get_pcmr_text())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total tokens in the final corpus: 3208853\n"
     ]
    }
   ],
   "source": [
    "def make_bigrams(normalized_tokens):\n",
    "    \"\"\" create bigrams froms normalized tokens corpus\"\"\"\n",
    "    bigram = gensim.models.Phrases(normalized_tokens, min_count = 18, threshold = 16)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in normalized_tokens]\n",
    "\n",
    "#min_count: ignore all words and bigrams with total collected count lower than this\n",
    "#threshold represents a score threshold for forming the phrases (higher means fewer phrases). A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold\n",
    "\n",
    "normalized_tokens_bigrams = make_bigrams(normalized_tokens)\n",
    "tokens = 0\n",
    "for i in normalized_tokens_bigrams:\n",
    "    tokens += len(i)\n",
    "print(f\"Total tokens in the final corpus: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "###############################################\n",
    "feature_size = 100    # word vector \n",
    "window_context = 10   # context window size i.e. maximum distance between current and predicted word within a sentence\n",
    "min_word_count = 36   # Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them\n",
    "sample = 1e-3         # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "learning_rate = 0.01  # the initial learning rate\n",
    "iterations = 20        # Number of iterations over the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Training Data and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2vec model creation and training completed in 477 seconds (7.95 min or 0.13 hours)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "w2v_model = Word2Vec(min_count = min_word_count,\n",
    "                     window = window_context,\n",
    "                     size = feature_size,\n",
    "                     sg = 1,\n",
    "                     sample = sample,\n",
    "                     negative = 3,\n",
    "                     iter = iterations,\n",
    "                     workers = 1)\n",
    "w2v_model.build_vocab(normalized_tokens_bigrams)\n",
    "w2v_model.train(normalized_tokens_bigrams, total_examples=w2v_model.corpus_count, epochs = w2v_model.iter)\n",
    "dur = round(time.time() - start_time)\n",
    "print(f\"Word2vec model creation and training completed in {dur} seconds ({round(dur / 60, 2)} min or {round(dur / 3600, 2)} hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Target Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_lst = ['physical_environment', 'soil', 'vegetation', 'water', 'fish', 'wetland', 'wildlife', 'species', 'air', 'air_quality', 'acoustic_environment', 'heritage', 'heritage_resource', 'access']\n",
    "sub_cat_vec_lst = ['erosion', 'coarse_fragment', 'subsidence', 'compaction', 'watercourse', 'invasive', 'plant', 'weed', 'rare', 'stream', 'riparian']\n",
    "vec_sub_cat = []\n",
    "vec_lst.extend(sub_cat_vec_lst)\n",
    "vec_sub_cat.extend(vec_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_word_dict = {}\n",
    "for root_word in vec_sub_cat:\n",
    "    try:\n",
    "        context_words = w2v_model.wv.most_similar(positive = [root_word],topn = 25)\n",
    "        root_word_dict[root_word] = context_words\n",
    "    except:\n",
    "        root_word_dict[root_word] = 'The word is not in vocabulary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_df = pd.DataFrame.from_dict(root_word_dict)\n",
    "word2vec_df.to_csv('word2vecembeddings.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_keys(dict, key1, key2):\n",
    "    \"\"\"Merge values of similar context words together\"\"\"\n",
    "    for context_word in dict[key1]:\n",
    "        dict[key2].append(context_word)\n",
    "    del dict[key1]\n",
    "    return dict\n",
    "\n",
    "def append_value(dict, key, value):\n",
    "    \"\"\"Append values in the dictionary\"\"\"\n",
    "    dict[key].append(value)\n",
    "    return dict\n",
    "\n",
    "def append_key_as_value(dict):\n",
    "    \"\"\"Append dictionary key as value\"\"\"\n",
    "    for key in dict:\n",
    "        dict[key].append((key, 1.0))\n",
    "    return dict\n",
    "\n",
    "def remove_underscores_duplicates(dict):\n",
    "    \"\"\"Remove underscores from the dictionary keys and values bigrams followed by removing duplicates from values\"\"\"\n",
    "    dict_final = {}\n",
    "    for key,value in dict.items():\n",
    "        new_key = key.replace('_', ' ')\n",
    "        new_value = [value[0].replace('_', ' ') for value in dict[key]]\n",
    "        dict_final[new_key] = new_value\n",
    "    return {key:list(set(value)) for key, value in dict_final.items()}\n",
    "\n",
    "def replace_keys(dict, old_keys, new_keys):\n",
    "    \"\"\"Replace some of the keys for the purpose of naming consistency in SQL database\"\"\"\n",
    "    for idx, new_key in enumerate(new_keys):\n",
    "        dict[new_key] = dict.pop((old_keys)[idx])\n",
    "    return dict\n",
    "\n",
    "def remove_dictionary_values(dictionary, vec, context_words):\n",
    "    \"\"\"Remove the context words which were incorrectly tagged to VECs in word2vec model\"\"\"\n",
    "    for key, value in dictionary.items():\n",
    "        if key == vec:\n",
    "            for word in context_words:\n",
    "                if word in value:\n",
    "                    value.remove(word)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegetation_context_words = ['wetland', 'subsidence', 'erosion', 'cover']\n",
    "water_context_words = ['erosion']\n",
    "wildlife_context_words = ['air quality', 'acoustic environment']\n",
    "air_context_words = ['acoustic environment', 'habitat', 'special status', 'wildlife', 'traditional land']\n",
    "heritage_context_words = ['acoustic environment', 'air quality']\n",
    "physical_context_words = ['weed', 'admix']\n",
    "wetlands_context_words = ['vegetation']\n",
    "acoustic_context_words = ['air quality', 'course fragment', 'wildlife']\n",
    "fish_context_words = ['channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    merge_keys(root_word_dict, 'erosion', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'coarse_fragment', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'subsidence', 'physical_environment')\n",
    "    merge_keys(root_word_dict, 'compaction', 'soil')\n",
    "    merge_keys(root_word_dict, 'invasive', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'plant', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'weed', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'rare', 'vegetation')\n",
    "    merge_keys(root_word_dict, 'watercourse', 'wetland')\n",
    "    merge_keys(root_word_dict, 'stream', 'wetland')\n",
    "    merge_keys(root_word_dict, 'riparian', 'wetland')\n",
    "    merge_keys(root_word_dict, 'heritage_resource', 'heritage')\n",
    "    merge_keys(root_word_dict, 'air_quality', 'air')\n",
    "    append_value(root_word_dict, 'physical_environment', ('coarse fragment', 1.0))\n",
    "    append_value(root_word_dict, 'soil', ('compaction', 1.0))\n",
    "    append_value(root_word_dict, 'physical_environment', ('crown', 1.0))\n",
    "    append_value(root_word_dict, 'vegetation', ('plant', 1.0))\n",
    "    append_value(root_word_dict, 'air', ('quality', 1.0))\n",
    "    append_value(root_word_dict, 'species', ('specie at risk', 1.0))\n",
    "    append_value(root_word_dict, 'vegetation', ('invasive', 1.0))\n",
    "    append_value(root_word_dict, 'wetland', ('watercourse', 1.0))\n",
    "    append_value(root_word_dict, 'access', ('navigation', 1.0))\n",
    "    append_key_as_value(root_word_dict)\n",
    "    dict_final = remove_underscores_duplicates(root_word_dict)\n",
    "    old_keys = ['physical environment','wetland', 'acoustic environment', 'access']\n",
    "    new_keys = ['physical', 'wetlands', 'acoustic', 'navigation']\n",
    "    replace_keys(dict_final, old_keys, new_keys)\n",
    "    remove_dictionary_values(dict_final, 'vegetation', vegetation_context_words)\n",
    "    remove_dictionary_values(dict_final, 'water', water_context_words)\n",
    "    remove_dictionary_values(dict_final, 'wildlife', wildlife_context_words)\n",
    "    remove_dictionary_values(dict_final, 'air', air_context_words)\n",
    "    remove_dictionary_values(dict_final, 'heritage', heritage_context_words)\n",
    "    remove_dictionary_values(dict_final, 'physical', physical_context_words)\n",
    "    remove_dictionary_values(dict_final, 'wetlands', wetlands_context_words)\n",
    "    remove_dictionary_values(dict_final, 'acoustic', acoustic_context_words)\n",
    "    remove_dictionary_values(dict_final, 'fish', fish_context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'soil': ['depth',\n",
       "  'crop growth',\n",
       "  'break level',\n",
       "  'minor',\n",
       "  'topography class',\n",
       "  'shovel resistance',\n",
       "  'cultivate varying',\n",
       "  'nearly level',\n",
       "  'rutting',\n",
       "  'severe',\n",
       "  'subsoil',\n",
       "  'productivity',\n",
       "  'degree compaction',\n",
       "  'soils',\n",
       "  'compaction rutting',\n",
       "  'gentle slope',\n",
       "  'discing',\n",
       "  'reduced crop',\n",
       "  'investigation site',\n",
       "  'topsoil differentiate',\n",
       "  'compaction',\n",
       "  'paratille pellet',\n",
       "  'texture',\n",
       "  'productivity topsoilsubsoil',\n",
       "  'unit depth',\n",
       "  'paratille',\n",
       "  'notation level',\n",
       "  'plant vigor',\n",
       "  'compaction rut',\n",
       "  'physical resistance',\n",
       "  'shovel test',\n",
       "  'aggregate size',\n",
       "  'sampling site',\n",
       "  'productivity lower',\n",
       "  'topsoil',\n",
       "  'alleviate compaction',\n",
       "  'reduce',\n",
       "  'unit',\n",
       "  'phase',\n",
       "  'warrant alleviate',\n",
       "  'admixing',\n",
       "  'shag topographic',\n",
       "  'admix',\n",
       "  'topsoil thickness',\n",
       "  'soil',\n",
       "  'inject',\n",
       "  'obo',\n",
       "  'bulk density',\n",
       "  'saobo',\n",
       "  'colour'],\n",
       " 'vegetation': ['growth',\n",
       "  'uncommon',\n",
       "  'specie',\n",
       "  'unsuccessful',\n",
       "  'special conservation',\n",
       "  'avoid artificial',\n",
       "  'cress',\n",
       "  'density',\n",
       "  'orchid',\n",
       "  'establishment',\n",
       "  'factor',\n",
       "  'flower',\n",
       "  'trefoil american',\n",
       "  'yellow sweet',\n",
       "  'grass forbs',\n",
       "  'noxious',\n",
       "  'establish seedling',\n",
       "  'wd woody',\n",
       "  'hand pick',\n",
       "  'sna exotic',\n",
       "  'ecological',\n",
       "  'species',\n",
       "  'poa pratensis',\n",
       "  'effectiveness',\n",
       "  'undesirable',\n",
       "  'row',\n",
       "  'spearleave goosefoot',\n",
       "  'equivalent ecosite',\n",
       "  'namely',\n",
       "  'herbaceous',\n",
       "  'monitor',\n",
       "  'manage',\n",
       "  'smooth brome',\n",
       "  'insect',\n",
       "  'woosaree',\n",
       "  'air shovel',\n",
       "  'scentless chamomile',\n",
       "  'noxious prohibit',\n",
       "  'crested wheatgrass',\n",
       "  'perennial',\n",
       "  'issue',\n",
       "  'treatment mound',\n",
       "  'acims',\n",
       "  'common burdock',\n",
       "  'rare plant',\n",
       "  'seral',\n",
       "  'concern',\n",
       "  'competitive',\n",
       "  'nuisance',\n",
       "  'kochia russian',\n",
       "  'plant',\n",
       "  'exhibit excellent',\n",
       "  'horizontal strata',\n",
       "  'longterm',\n",
       "  'seed catch',\n",
       "  'undesirable specie',\n",
       "  'invasive',\n",
       "  'seedle',\n",
       "  'shift',\n",
       "  'desirable',\n",
       "  'thistle',\n",
       "  'low',\n",
       "  'density hard',\n",
       "  'narrow workspace',\n",
       "  'golden saxifrage',\n",
       "  'occurrence',\n",
       "  'enhancement avoid',\n",
       "  'unique',\n",
       "  'appropriate situation',\n",
       "  'sp mound',\n",
       "  'weed',\n",
       "  'moderate',\n",
       "  'line yet',\n",
       "  'widespread',\n",
       "  'mowed',\n",
       "  'psa mitigation',\n",
       "  'mow',\n",
       "  'chemical',\n",
       "  'majority',\n",
       "  'decrease percent',\n",
       "  'western jacobsladder',\n",
       "  'plain rough',\n",
       "  'pcm failure',\n",
       "  'throughout',\n",
       "  'white cockle',\n",
       "  'vegetation',\n",
       "  'compare offrow',\n",
       "  'ecological integrity',\n",
       "  'downy brome',\n",
       "  'spray',\n",
       "  'follow nonchemical',\n",
       "  'shift ditchline',\n",
       "  'control',\n",
       "  'individual',\n",
       "  'andor aggressive',\n",
       "  'patch',\n",
       "  'particularly trench',\n",
       "  'observe',\n",
       "  'compare',\n",
       "  'rightofway',\n",
       "  'health vigour',\n",
       "  'seedle planting',\n",
       "  'composition',\n",
       "  'saraliste',\n",
       "  'prevention',\n",
       "  'ecological community',\n",
       "  'establish',\n",
       "  'species longer',\n",
       "  'infestation',\n",
       "  'absinthe',\n",
       "  'single',\n",
       "  'prohibit noxious',\n",
       "  'false ragweed',\n",
       "  'desirable specie',\n",
       "  'threat',\n",
       "  'somewhat',\n",
       "  'pigweed'],\n",
       " 'water': ['exhibit fully',\n",
       "  'ponding',\n",
       "  'contouring daylighte',\n",
       "  'artificial',\n",
       "  'saturation',\n",
       "  'aenv',\n",
       "  'well aesrd',\n",
       "  'nutrient',\n",
       "  'turbidity',\n",
       "  'quality',\n",
       "  'cap function',\n",
       "  'exhibit semisaturate',\n",
       "  'hydrocarbon',\n",
       "  'fullysaturated',\n",
       "  'groundwater',\n",
       "  'submergent',\n",
       "  'pool',\n",
       "  'flow',\n",
       "  'discharge',\n",
       "  'open',\n",
       "  'body',\n",
       "  'water',\n",
       "  'release',\n",
       "  'surface',\n",
       "  'freeze',\n",
       "  'pouch'],\n",
       " 'fish': ['white sucker',\n",
       "  'sport',\n",
       "  'dam pump',\n",
       "  'aar',\n",
       "  'restored',\n",
       "  'fisheries',\n",
       "  'northern pike',\n",
       "  'dryfrozen',\n",
       "  'rear',\n",
       "  'instream',\n",
       "  'nonfish bearing',\n",
       "  'fn tributary',\n",
       "  'spawn',\n",
       "  'isolation',\n",
       "  'isolate trenched',\n",
       "  'bear',\n",
       "  'fish habitat',\n",
       "  'pool',\n",
       "  'downstream',\n",
       "  'amphibian',\n",
       "  'stream',\n",
       "  'brook stickleback',\n",
       "  'adhere instream',\n",
       "  'aquatic',\n",
       "  'fish'],\n",
       " 'wildlife': ['cosewic',\n",
       "  'ag er',\n",
       "  'wildlife',\n",
       "  'con str',\n",
       "  'mammal',\n",
       "  'topsoil compactionrutte',\n",
       "  'ucti mi',\n",
       "  'heritage andtraditional',\n",
       "  'mea sure',\n",
       "  'fish fish',\n",
       "  'trench bedrockslope',\n",
       "  'special status',\n",
       "  'status endanger',\n",
       "  'risk',\n",
       "  'reclamation os',\n",
       "  'heritage traditional',\n",
       "  'aic sc',\n",
       "  'legend rev',\n",
       "  'westland resource',\n",
       "  'edure',\n",
       "  'mitig ation',\n",
       "  'description draw',\n",
       "  'topsoil salvageproc',\n",
       "  'habitat'],\n",
       " 'species': ['reindeer lichen',\n",
       "  'specie at risk',\n",
       "  'specie',\n",
       "  'rough hair',\n",
       "  'red raspberry',\n",
       "  'slender wheatgrass',\n",
       "  'astragalus',\n",
       "  'kentucky bluegrass',\n",
       "  'exotic',\n",
       "  'harebell',\n",
       "  'moss',\n",
       "  'wild',\n",
       "  'acims',\n",
       "  'nonnative',\n",
       "  'wild strawberry',\n",
       "  'species',\n",
       "  'forbs',\n",
       "  'canada buffaloberry',\n",
       "  'beautiful sedge',\n",
       "  'cinquefoil',\n",
       "  'common horsetail',\n",
       "  'canadensis',\n",
       "  'wild rye',\n",
       "  'bromus',\n",
       "  'fowl bluegrass',\n",
       "  'rubus',\n",
       "  'slender'],\n",
       " 'air': ['equipment',\n",
       "  'travel',\n",
       "  'therefore',\n",
       "  'construct',\n",
       "  'applicator',\n",
       "  'biophysical',\n",
       "  'respect',\n",
       "  'consultation',\n",
       "  'equipment clean',\n",
       "  'forward',\n",
       "  'power washmist',\n",
       "  'prior enter',\n",
       "  'instability fill',\n",
       "  'air epp',\n",
       "  'impact',\n",
       "  'activity',\n",
       "  'shovel compress',\n",
       "  'resources',\n",
       "  'fish fish',\n",
       "  'heritage resource',\n",
       "  'quality',\n",
       "  'potential',\n",
       "  'human occupancy',\n",
       "  'ramp geotec',\n",
       "  'move',\n",
       "  'full wash',\n",
       "  'arrive',\n",
       "  'cleaning station',\n",
       "  'risk',\n",
       "  'part',\n",
       "  'vehicle',\n",
       "  'air',\n",
       "  'proceed',\n",
       "  'dust',\n",
       "  'clean',\n",
       "  'wash',\n",
       "  'shoofly',\n",
       "  'variance minor',\n",
       "  'initial secondary',\n",
       "  'prior entry',\n",
       "  'setup',\n",
       "  'traffic',\n",
       "  'farm',\n",
       "  'cleaning',\n",
       "  'residual effect',\n",
       "  'study',\n",
       "  'organic farm'],\n",
       " 'heritage': ['manitoba dept',\n",
       "  'corporation saskatchewan',\n",
       "  'oct oct',\n",
       "  'loreburn',\n",
       "  'historical resources',\n",
       "  'woodlot',\n",
       "  'exist',\n",
       "  'branch',\n",
       "  'mar',\n",
       "  'sport',\n",
       "  'conservation',\n",
       "  'craik',\n",
       "  'fw fwtwl',\n",
       "  'resources',\n",
       "  'heritage resource',\n",
       "  'credit',\n",
       "  'archaeological',\n",
       "  'conflict',\n",
       "  'discovery',\n",
       "  'important bird',\n",
       "  'administration',\n",
       "  'human occupancy',\n",
       "  'special status',\n",
       "  'risk',\n",
       "  'stewardship',\n",
       "  'mile post',\n",
       "  'conservation data',\n",
       "  'heritage',\n",
       "  'fwfw',\n",
       "  'road network',\n",
       "  'conservation prairie',\n",
       "  'slope strong',\n",
       "  'historical',\n",
       "  'farm rehabilitation',\n",
       "  'property',\n",
       "  'culture',\n",
       "  'municipality boundary',\n",
       "  'cultural',\n",
       "  'act clearance',\n",
       "  'route mile',\n",
       "  'location',\n",
       "  'road',\n",
       "  'shoofly temporary',\n",
       "  'ig iv'],\n",
       " 'physical': ['nrsm sm',\n",
       "  'potential adverse',\n",
       "  'ponde water',\n",
       "  'fragment',\n",
       "  'programs',\n",
       "  'physical environment',\n",
       "  'course fragment',\n",
       "  'point horizon',\n",
       "  'degree excess',\n",
       "  'datum collection',\n",
       "  'picking',\n",
       "  'gully',\n",
       "  'minor rill',\n",
       "  'trench crown',\n",
       "  'visual evaluation',\n",
       "  'landscape',\n",
       "  'silt fence',\n",
       "  'rv channel',\n",
       "  'el soils',\n",
       "  'coarse fragment',\n",
       "  'drawing epp',\n",
       "  'crop growth',\n",
       "  'minor',\n",
       "  'screw anchor',\n",
       "  'contouring',\n",
       "  'rutting',\n",
       "  'sedimentation',\n",
       "  'landscape feature',\n",
       "  'issue',\n",
       "  'stone',\n",
       "  'hillslope',\n",
       "  'spring overflight',\n",
       "  'due snow',\n",
       "  'diversion berm',\n",
       "  'rilling',\n",
       "  'roughness',\n",
       "  'grower certify',\n",
       "  'con slopeaspect',\n",
       "  'methodology preliminary',\n",
       "  'follow subsection',\n",
       "  'ponde',\n",
       "  'resolution status',\n",
       "  'rillsgullie',\n",
       "  'contour restoration',\n",
       "  'topsoilsubsoil',\n",
       "  'slumping',\n",
       "  'erosion',\n",
       "  'cf consistency',\n",
       "  'three subcategorie',\n",
       "  'microtopography',\n",
       "  'appropriately',\n",
       "  'drainage',\n",
       "  'nolf',\n",
       "  'aio',\n",
       "  'stone gravel',\n",
       "  'stoniness',\n",
       "  'nolf nolf',\n",
       "  'control',\n",
       "  'subsidence',\n",
       "  'sediment',\n",
       "  'ponding',\n",
       "  'data collection',\n",
       "  'rill',\n",
       "  'cattle exclusion',\n",
       "  'instability fill',\n",
       "  'rut',\n",
       "  'productivity',\n",
       "  'long wide',\n",
       "  'contour',\n",
       "  'woody debris',\n",
       "  'slope',\n",
       "  'crown',\n",
       "  'recommendation',\n",
       "  'quality quantity',\n",
       "  'debris',\n",
       "  'nr sm',\n",
       "  'restriction restr',\n",
       "  'inhibit recovery',\n",
       "  'excess coarse',\n",
       "  'repair',\n",
       "  'cross drain',\n",
       "  'etc',\n",
       "  'slump',\n",
       "  'ponding screw'],\n",
       " 'wetlands': ['nvc',\n",
       "  'time tertiary',\n",
       "  'watercourse',\n",
       "  'scour',\n",
       "  'function',\n",
       "  'thimbleberry',\n",
       "  'dryfrozen',\n",
       "  'irrigation',\n",
       "  'trench bedrockslope',\n",
       "  'green alder',\n",
       "  'opencut',\n",
       "  'tss',\n",
       "  'culvert',\n",
       "  'fn tributary',\n",
       "  'functionality',\n",
       "  'hydrophytic',\n",
       "  'row',\n",
       "  'tributary dennis',\n",
       "  'crossing',\n",
       "  'stream',\n",
       "  'weedy',\n",
       "  'riprap',\n",
       "  'condition',\n",
       "  'representative hydrologic',\n",
       "  'via uniform',\n",
       "  'hydrological',\n",
       "  'sa ska',\n",
       "  'dennis stream',\n",
       "  'capability uniform',\n",
       "  'epi cromer',\n",
       "  'woody',\n",
       "  'net',\n",
       "  'turbidity',\n",
       "  'exhibit semisaturate',\n",
       "  'bed bank',\n",
       "  'bridge',\n",
       "  'emergent',\n",
       "  'irrigate',\n",
       "  'facility city',\n",
       "  'wrap',\n",
       "  'isolate trenched',\n",
       "  'flow',\n",
       "  'downstream',\n",
       "  'sedge bulrush',\n",
       "  'cross',\n",
       "  'contain emergent',\n",
       "  'footprint avoidance',\n",
       "  'maintain hydrology',\n",
       "  'deposition',\n",
       "  'dam pump',\n",
       "  'functional',\n",
       "  'health ranking',\n",
       "  'tch ew',\n",
       "  'diversity maintain',\n",
       "  'instream',\n",
       "  'nonfish bearing',\n",
       "  'applicationand planning',\n",
       "  'native',\n",
       "  'channel',\n",
       "  'native blend',\n",
       "  'willow',\n",
       "  'tributary',\n",
       "  'substrate',\n",
       "  'mitig ation',\n",
       "  'hadd',\n",
       "  'inspection form',\n",
       "  'treed swamp',\n",
       "  'flood',\n",
       "  'net loss',\n",
       "  'fish',\n",
       "  'wc',\n",
       "  'creek',\n",
       "  'ripariancult',\n",
       "  'facility eh',\n",
       "  'bank approach',\n",
       "  'riparian planting',\n",
       "  'bed',\n",
       "  'ntzs adjacent',\n",
       "  'sufficient break',\n",
       "  'baseline',\n",
       "  'rainbow',\n",
       "  'pipe installation',\n",
       "  'proper functional',\n",
       "  'hydrology maintain',\n",
       "  'wetland',\n",
       "  'bank',\n",
       "  'mitig ationmea',\n",
       "  'healthy uniform',\n",
       "  'stabilization',\n",
       "  'upstream',\n",
       "  'topsoil salvageproc',\n",
       "  'dry',\n",
       "  'alteration',\n",
       "  'emergent submergent'],\n",
       " 'acoustic': ['therefore',\n",
       "  'acoustic environment',\n",
       "  'biophysical',\n",
       "  'respect',\n",
       "  'consultation',\n",
       "  'instability fill',\n",
       "  'impact',\n",
       "  'fish fish',\n",
       "  'resources',\n",
       "  'heritage resource',\n",
       "  'traditional land',\n",
       "  'potential',\n",
       "  'human occupancy',\n",
       "  'special status',\n",
       "  'risk',\n",
       "  'part',\n",
       "  'dust',\n",
       "  'plant vigor',\n",
       "  'initial secondary',\n",
       "  'variance minor',\n",
       "  'warrant alleviate',\n",
       "  'noise',\n",
       "  'study',\n",
       "  'habitat'],\n",
       " 'navigation': ['prior',\n",
       "  'atv',\n",
       "  'need notify',\n",
       "  'deck',\n",
       "  'access',\n",
       "  'treatment mound',\n",
       "  'permission',\n",
       "  'spring',\n",
       "  'corduroy',\n",
       "  'line sight',\n",
       "  'rollback',\n",
       "  'ineffective',\n",
       "  'contact tenant',\n",
       "  'mounding',\n",
       "  'esrd tenant',\n",
       "  'trail',\n",
       "  'mound',\n",
       "  'landowner',\n",
       "  'navigation',\n",
       "  'traffic',\n",
       "  'maintain contact',\n",
       "  'centreline log',\n",
       "  'shoofly temporary',\n",
       "  'qp baseline',\n",
       "  'road',\n",
       "  'revegetation',\n",
       "  'lineofsight']}"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "dict_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "# Importing environmental variables library that reads from the .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading key-value pairs from the .env file into the OS environment\n",
    "load_dotenv()\n",
    "\n",
    "# Reading the key-value pairs from the OS environment\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASS\")\n",
    "db_hostname = os.getenv(\"DB_HOST\")\n",
    "db_name = os.getenv(\"DB_DATABASE\")\n",
    "\n",
    "# Using those variables in the connection string using \"F\" strings\n",
    "conn_string = f\"mysql+mysqldb://{user}:{password}@{db_hostname}/{db_name}?charset=utf8mb4\"\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"SELECT distinct i.tableId, i.rowIndex, i.vec_pri, i.vec_sec, i.status_bin, i.status, i.status_txt, i.issue_pri, i.issue_sec, p.company, p.monitoring_year, pc.consultantName, pr.application_title_short, w.word2vec_vec FROM issues i LEFT JOIN word2vec w ON i.tableId = w.tableId AND i.rowIndex = w.rowIndex LEFT JOIN locations l ON i.tableId = l.tableId and i.rowIndex = l.rowIndex LEFT JOIN tables t ON i.tableId = t.headTable LEFT JOIN pdfs p ON t.pdfName = p.pdfName LEFT JOIN projects pr ON p.application_id = pr.application_id LEFT JOIN pdfsconsultants pc ON p.pdfName = pc.pdfName LEFT JOIN tables_tags tt ON t.tableId = tt.tableId WHERE locFormat = 'DLS' AND i.issue_pri NOT IN ('-', '?', 'ESC') AND CONCAT(i.issue_pri,i.issue_sec) <> '----' AND CONCAT(i.issue_pri,i.issue_sec) <> '' AND status_bin NOT IN ('--', '-', '') AND status_bin IS NOT NULL;\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df1 = df.copy()\n",
    "    df = df.applymap(lambda x: x.strip() if type(x)==str else x) # delete whitespaces\n",
    "    df.vec_pri = df.vec_pri.replace('\\s+', ' ', regex=True) # delete extra space between text strings\n",
    "    df.vec_psec = df.vec_sec.replace('\\s+', ' ', regex=True)\n",
    "    df.vec_pri = df.vec_pri.str.lower()\n",
    "    df.vec_sec = df.vec_sec.str.lower()\n",
    "    df['vec_pri'].fillna('', inplace = True)\n",
    "    df['vec_sec'].fillna('', inplace = True)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(remove_between_delimiters)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(remove_between_delimiters)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(remove_special_characters, remove_digits = True)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(remove_special_characters, remove_digits = True)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(lemmatize_text)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(lemmatize_text)\n",
    "    df['vec_pri'] = df['vec_pri'].apply(prohibitedWords)\n",
    "    df['vec_sec'] = df['vec_sec'].apply(prohibitedWords)\n",
    "#df.loc[df.vec_pri.str.contains(\"(?i)physical environment\", na = False), 'physical'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_keyword_count = []\n",
    "vec_keywords = []\n",
    "\n",
    "for index, row in enumerate(df.itertuples()):\n",
    "    issue_keyword_count = []\n",
    "    \n",
    "    for key, value in dict_final.items():\n",
    "        counter = 0\n",
    "        keyword = []\n",
    "        for vec in value:\n",
    "            if re.search(r'\\b' + vec + r'\\b', row.vec_pri):\n",
    "                keyword.append(vec)\n",
    "                counter += 1\n",
    "        issue_keyword_count.append(counter)\n",
    "        vec_keywords.append(keyword)\n",
    "        \n",
    "    if sum(issue_keyword_count) == 0:\n",
    "        issue_keyword_count = []\n",
    "        keyword = []\n",
    "        for key, value in dict_final.items():\n",
    "            idx = 0\n",
    "            for vec in value:\n",
    "                if re.search(r'\\b' + vec + r'\\b', row.vec_sec):\n",
    "                    keyword.append(vec)\n",
    "                    idx += 1\n",
    "            issue_keyword_count.append(idx)\n",
    "        vec_keywords.append(keyword)\n",
    "            \n",
    "    vec_keyword_count.append(issue_keyword_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      soil  vegetation  water  fish  wildlife  species  air  heritage  \\\n",
       "0        2           1      0     0         0        0    0         0   \n",
       "1        2           1      0     0         0        0    0         0   \n",
       "2        1           2      0     0         0        0    0         0   \n",
       "3        0           2      0     0         0        0    0         0   \n",
       "4        0           2      0     0         0        0    0         0   \n",
       "...    ...         ...    ...   ...       ...      ...  ...       ...   \n",
       "1888     0           0      0     0         0        0    0         0   \n",
       "1889     0           2      0     0         0        0    0         0   \n",
       "1890     0           2      0     0         0        0    0         0   \n",
       "1891     0           0      1     0         0        0    0         0   \n",
       "1892     0           0      0     0         0        0    0         0   \n",
       "\n",
       "      physical  wetlands  acoustic  navigation  \n",
       "0            2         0         0           1  \n",
       "1            2         0         0           1  \n",
       "2            1         0         0           0  \n",
       "3            0         0         0           0  \n",
       "4            0         0         0           0  \n",
       "...        ...       ...       ...         ...  \n",
       "1888         0         1         0           0  \n",
       "1889         0         0         0           0  \n",
       "1890         0         0         0           0  \n",
       "1891         1         0         0           0  \n",
       "1892         1         0         0           0  \n",
       "\n",
       "[1893 rows x 12 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>soil</th>\n      <th>vegetation</th>\n      <th>water</th>\n      <th>fish</th>\n      <th>wildlife</th>\n      <th>species</th>\n      <th>air</th>\n      <th>heritage</th>\n      <th>physical</th>\n      <th>wetlands</th>\n      <th>acoustic</th>\n      <th>navigation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>1888</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1889</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1891</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1892</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1893 rows × 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "# Create the pandas DataFrame  \n",
    "df2 = pd.DataFrame(vec_keyword_count, columns = dict_final.keys()) \n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3 = pd.concat([df1, df2], axis = 1)\n",
    "df3['VECassigned'] = df3[['soil', 'vegetation', 'water', 'fish', 'wildlife', 'species', 'air','heritage', 'physical', 'wetlands', 'acoustic', 'navigation']].idxmax(axis=1)\n",
    "df3['Sum'] = df3[['soil', 'vegetation', 'water', 'fish', 'wildlife', 'species', 'air','heritage', 'physical', 'wetlands', 'acoustic', 'navigation']].sum(axis=1)\n",
    "df3.loc[df3.Sum == 0, 'VECassigned'] = 'No VEC Assigned'\n",
    "df3.drop('Sum', axis=1, inplace=True)\n",
    "# for idx, row in df2.iterrows():\n",
    "#     if  df2.loc[idx,'vec_simple'] == \"generic\":\n",
    "#         df2.loc[idx,'VECassigned'] = \"generic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('issues_flatFilev1.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "'''def populate_word2vec_table():\n",
    "    data = df3.to_dict('records')\n",
    "    insert_query = 'INSERT INTO word2vec (word2vec_vec, tableId, rowIndex) VALUE (%s, %s, %s);'\n",
    "    with engine.connect() as conn:\n",
    "        for item in data:\n",
    "            conn.execute(insert_query, (item['VECassigned'], item['tableId'], item['rowIndex']))\n",
    "    print(\"Done\") ## If it fails to insert all rows, it could be because of foreign key constraint error. Refer this link: https://stackoverflow.com/questions/2965837/insert-statement-conflicted-with-the-foreign-key-constraint-sql-server\n",
    "populate_word2vec_table()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df = pd.read_csv('file.csv', encoding='cp1252')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df = df.applymap(lambda x: x.strip() if type(x)==str else x) # delete whitespaces\n",
    "df.filing_manual_text = df.filing_manual_text.replace('\\s+', ' ', regex=True) # delete extra space between text strings\n",
    "df.filing_manual_text = df.filing_manual_text.str.lower()\n",
    "df['filing_manual_text'] = df['filing_manual_text'].apply(lemmatize_text)\n",
    "df['filing_manual_text'] = df['filing_manual_text'].apply(prohibitedWords)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_keyword_count = []\n",
    "vec_keywords = []\n",
    "\n",
    "for index, row in enumerate(df.itertuples()):\n",
    "    issue_keyword_count = []\n",
    "    \n",
    "    for key, value in dict_final.items():\n",
    "        counter = 0\n",
    "        keyword = []\n",
    "        for vec in value:\n",
    "            if re.search(r'\\b' + vec + r'\\b', row.filing_manual_text):\n",
    "                keyword.append(vec)\n",
    "                counter += 1\n",
    "        issue_keyword_count.append(counter)\n",
    "        vec_keywords.append(keyword)\n",
    "            \n",
    "    vec_keyword_count.append(issue_keyword_count)\n",
    "\n",
    "vec_keyword_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualizing the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn = 2)] for search_term in ['physical', 'soil', 'erosion', 'vegetation', 'water', 'fish', 'wetland', 'wildlife', 'specie', 'air']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Create TSNE model and plot it\"\n",
    "    words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "    wvs = w2v_model.wv[words]\n",
    "    tsne_model = TSNE(perplexity = 2, n_components = 2, n_iter = 10000, random_state = 0)\n",
    "    np.set_printoptions(suppress = True)\n",
    "    T = tsne_model.fit_transform(wvs)\n",
    "    labels = words\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.scatter(T[:, 0], T[:, 1], c = 'orange', edgecolors = 'r')\n",
    "    for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "        plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit44ddb9ad7b944aa98606efee99bf806f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}