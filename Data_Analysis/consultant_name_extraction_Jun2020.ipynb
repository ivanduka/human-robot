{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text, create_engine\n",
    "import pandas as pd\n",
    "from PIL import Image \n",
    "import pytesseract \n",
    "import sys \n",
    "from pdf2image import convert_from_path \n",
    "import os\n",
    "import numpy as np \n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing environmental variables library that reads from the .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading key-value pairs from the .env file into the OS environment\n",
    "load_dotenv()\n",
    "\n",
    "# Reading the key-value pairs from the OS environment\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASS\")\n",
    "db_hostname = os.getenv(\"DB_HOST\")\n",
    "db_name = os.getenv(\"DB_DATABASE\")\n",
    "\n",
    "# Using those variables in the connection string using \"F\" strings\n",
    "conn_string = f\"mysql+mysqldb://{user}:{password}@{db_hostname}/{db_name}?charset=utf8mb4\"\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultant_name_df = pd.read_excel('G:/Post Construction/metadata csvs/consultants.xlsx', encoding = 'utf-8-sig')\n",
    "consultant_name_lst = consultant_name_df[\"consultant_name\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn: ## with context manager with, we don't have to close the connection everytime we run queries\n",
    "    query1 = \"SELECT * FROM pdfs;\"\n",
    "    df1 = pd.read_sql(query1, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "main_consultant_name = []\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\t1nipun\\AppData\\Local\\Tesseract-OCR\\tesseract.exe'\n",
    "for file in df1['pdfName'][:3]:\n",
    "    pages = convert_from_path(\"G:/Post Construction/PCMR_All/\" + file + '.pdf', 500, last_page= 2)\n",
    "    page = pages[i]\n",
    "        page.save(\"C:/Users/t1nipun/Desktop/PCMR/human-robot/Data_Analysis/pdf_firstpage_img/\" + filename, 'JPEG')\n",
    "    text = str(((pytesseract.image_to_string(Image.open(\"C:/Users/t1nipun/Desktop/PCMR/human-robot/Data_Analysis/pdf_firstpage_img/\" + filename))))).lower()\n",
    "    consultant_name = []\n",
    "    for name in consultant_name_lst:\n",
    "        if name.lower() in text:\n",
    "            consultant_name.append(name)\n",
    "    main_consultant_name.append(consultant_name)\n",
    "end = time.time()\n",
    "print(f'Runtime is {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "») eg 4 \\ x ws environmental suite 300, 805 - 8 avenue t: 403 269-5150\niting ltd. sw f: 403 269-5245\nconsult g = calgary, alberta www.axys.net\n\ncanada t2p 1h7\n\nencana ekwan pipeline project\npost construction monitoring\n\nprepared by\naxys environmental consulting ltd.\n\nfor\nencana ekwan pipeline inc.\n\nseptember 2004table of contents\n\n1 bacckkqrounnd. .........cccceeceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeneeeeneeeseeeeseeeseseeseeeaaseeseseeeseeneseesssenesenaeneeessenenes 2\n2 row condition .........ceccccccccecce sce e eee eeeeeeeeeeeae essen eseaeesaneseaeecea seen ecea essa saeaessanssenessanesenessnees 3\n3 vegetation recovery on the row.............cc:ccccesseeseeeeeeeeeeeeeeeeeeeeeseneeeeneeeneesneeeneeseneneaes 3\n3.1 general recovety .........cccccccccceeccceeeeeseeeeeeeeeeeeeeeeseeeesaeeesseeeseseesaeeeeseeeeeaeeeeeeeesaeeeesees 3\n3.2 recovery at stream crosssings ..........ccccceecceceeeeceeeeeeeeeeeeeeeeseeeeeaeeeeseeeeeseeeesaeeeeneeeesaes 4\n3.3 recovery in “minimal disturbance” zones ..............ccccccccseeceeececeeeeseeeceeeeseeeeaeeeseeeeaes 5\n3.3.1 method 20... cccccccecccececeeeceeeceeeseeseeeseeeee eee eeseeeseeeseeseeesaeeseeeseesaeesaeeseeeseesaes 5\n\n3.3.2 resus 2.0... cece cece cece eeeceeece eee eeseeeseeeeeeceeeeeeeeseeseeeseeeseeesaeeseeeeeesaeesseeeeeeseeegaes 6\n\n3.4 noxious weems.......... ccc cece cece eecceeeeeeeceeeceeeeeeeceeese esse eeseeeseeeseeeeeeeaeesaueceeesaeesseeseeeseeeges 6\n\n4 beetle occurrences in large diameter spruce on the row................:cccccssseeeeeeeeeees 8\n5 slash accumulations at deck sites .............cccceeseeeeeeeeeeeeeeeeeeeeeeeeeseneeseeeeeseneseeneeseenensoes 9\nbg summa 1... eee ceeecceeee eee eee eee eneeeeeeeeeeneseeaeeeees esses esses eeseeneeeesa sees eeeeeseeaeeeegeseeneeenseesonsessanees 9\n\nlist of tables\n\ntable 1 summary of ground cover at three sites on the ekwan pipeline project,\naugust 2004... occ cc ccccecccceeccceeececeeeeeseeeeeeeeceeeecessecesaeeeeseaeesseeeeseeeessueeeseeeesaeeeeas 6\n\nlist of figures\n\nfigure 1 distribution of daubenmire cover classes on the ekwan pipeline project,\naugust 2004.0... ec cceccccceeeeeeeeeeeee see eeecaeeeeeeseeeeeseeeeeeseaeeesseeeeeeseeeesaeeessaaeeesaaeeees 4\n\npage i d)) eg\nRuntime is 29.405860424041748\n"
    }
   ],
   "source": [
    "start = time.time()\n",
    "main_consultant_name = []\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\t1nipun\\AppData\\Local\\Tesseract-OCR\\tesseract.exe'\n",
    "for file in df1['pdfName'][:1]:\n",
    "    pages = convert_from_path(\"G:/Post Construction/PCMR_All/\" + file + '.pdf', 500)\n",
    "    text = \"\"\n",
    "    for i in range(0,2):\n",
    "        pages[i].save(\"C:/Users/t1nipun/Desktop/PCMR/human-robot/Data_Analysis/pdf_firstpage_img/\" + file +'_{}.jpg'.format(i), 'JPEG')\n",
    "        text += pytesseract.image_to_string(Image.open(\"C:/Users/t1nipun/Desktop/PCMR/human-robot/Data_Analysis/pdf_firstpage_img/\" + file +'_{}.jpg'.format(i))).lower()\n",
    "        consultant_name = []\n",
    "        for name in consultant_name_lst:\n",
    "            if name.lower() in text:\n",
    "                consultant_name.append(name)\n",
    "        main_consultant_name.append(consultant_name)\n",
    "end = time.time()\n",
    "print(f'Runtime is {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['consultant_name'] = main_consultant_name\n",
    "df2 = df1.copy()\n",
    "df2\n",
    "mapping_dict = df2.groupby('filingId')['consultant_name'].apply(lambda x: x.values.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst_of_lsts):\n",
    "    output = []\n",
    "    for element in lst_of_lsts:\n",
    "        if type(element) == list:\n",
    "            output.extend(element)\n",
    "    return list(set(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newdict = {key: flatten(value) for key, value in mapping_dict.items()}\n",
    "newdict[775971] = ['Golder Associates']\n",
    "newdict[786189] = ['TERA Environmental Consultants']\n",
    "df2['consultant_name'] = df2['filingId'].map(newdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_name = []\n",
    "main_consultant_name_step2 = []\n",
    "for line, row in enumerate(df2.itertuples(),1):  ## use head method with dataframe in order to slice the data when using itertuples() example df1.head(2).itertuples()\n",
    "    if len(row.consultant_name) == 0:\n",
    "        pdf_name.append(row.pdfName)   \n",
    "        text = row.xmlContent.lower().strip().split(\"table of contents\")\n",
    "        text_tuple = '_'.join(text[:1]), '_'.join(text[1:])\n",
    "        text_before_TOC = text_tuple[0]\n",
    "        consultant_name_step2 = []\n",
    "        for name in consultant_name_lst:\n",
    "            if name.lower() in text_before_TOC:\n",
    "                consultant_name_step2.append(name)\n",
    "        main_consultant_name_step2.append(consultant_name_step2)\n",
    "                #df1.set_value(row.Index, 'consultant_name', name) # reference: https://stackoverflow.com/questions/43222878/iterate-over-pandas-dataframe-and-update-the-value-attributeerror-cant-set-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapped = list(zip(pdf_name,main_consultant_name_step2))\n",
    "df3 = pd.DataFrame(mapped, columns = ['pdf_name','consultant_name'])\n",
    "df3.to_csv('random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  = ['pdfId', 'pdfName', 'filingId', 'totalPages', 'application_id', 'submitter', 'company', 'consultant_name']\n",
    "df1.to_csv('check.csv', encoding = 'utf-8-sig', columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}