{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.max_length = 6000000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing environmental variables library that reads from the .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading key-value pairs from the .env file into the OS environment\n",
    "load_dotenv()\n",
    "\n",
    "# Reading the key-value pairs from the OS environment\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASS\")\n",
    "db_hostname = os.getenv(\"DB_HOST\")\n",
    "db_name = os.getenv(\"DB_DATABASE\")\n",
    "\n",
    "# Using those variables in the connection string using \"F\" strings\n",
    "conn_string = f\"mysql+mysqldb://{user}:{password}@{db_hostname}/{db_name}?charset=utf8mb4\"\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmaspacy(my_new_str):\n",
    "    # nlp = en_core_web_sm.load()\n",
    "    # nlp.max_length = 6000000\n",
    "    sentence = my_new_str\n",
    "    doc = nlp(sentence)\n",
    "    return \" \".join([token.lemma_ for token in doc]) # joining all the word tokens after lemmatizer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM flat_data WHERE rating_coding = '';\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql(query, conn)\n",
    "    pattern = '|'.join(['<s>', '</s>'])\n",
    "    df['table_content'] = df['table_content'].str.replace(pattern, '')\n",
    "dic = {}\n",
    "\n",
    "for row in df.itertuples():\n",
    "    # converting JSON string to a list of lists of strings\n",
    "    table = json.loads(row.table_content)\n",
    "    headers = table[0]  # column headers  \n",
    "    for header in headers:\n",
    "        header = lemmaspacy(header)\n",
    "        if header in dic:\n",
    "            dic[header] += 1\n",
    "        else:\n",
    "            dic[header] = 1\n",
    "\n",
    "my_list = [(header, count) for header, count in dic.items()]  # Converting to list\n",
    "my_list.sort(key=lambda tup: tup[1], reverse=True)  # sorting the list\n",
    "\n",
    "total_headers = 0\n",
    "print(f\"COUNT\\tHEADER\")\n",
    "for item in my_list:\n",
    "    total_headers += item[1]\n",
    "    print(f\"{item[1]}\\t{item[0]}\")\n",
    "print()\n",
    "print(f'Total headers: {total_headers}; unique headers: {len(my_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for row in df.itertuples():\n",
    "    # converting JSON string to a list of lists of strings\n",
    "    table = json.loads(row.table_content)\n",
    "    print(type(table))\n",
    "    lst_element1 = \"SPREAD\"\n",
    "    lst_element2 = \"QUARTER SECTION\"\n",
    "    if lst_element1 in table[0][0] and lst_element2 in table[0][1]:\n",
    "        print(row.tableId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for row in df.itertuples():\n",
    "    table = json.loads(row.table_content)\n",
    "    if table[0][len(table[0])-1] == \"Topic\":\n",
    "        data = pd.DataFrame(table)\n",
    "        data.to_csv(r\"C:\\Users\\t1nipun\\Desktop\\PCMR\\human-robot\\Data_Analysis\\csvs\\\\\" + row.tableId + '.csv', encoding = 'utf-8-sig', index = False, header = None)\n",
    "    else:\n",
    "        table[0].extend(['VEC', 'GIS', 'Topic'])\n",
    "        for i in range(1, len(table)-1):\n",
    "            table[i].extend(['','',''])\n",
    "        data = pd.DataFrame(table)\n",
    "        data.to_csv(r\"C:\\Users\\t1nipun\\Desktop\\PCMR\\human-robot\\Data_Analysis\\csvs\\\\\" + row.tableId + '.csv', encoding = 'utf-8-sig', index = False, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.itertuples():\n",
    "    table = json.loads(row.table_content)\n",
    "    if table[0][len(table[0])-1] == \"Topic\":\n",
    "        data = pd.DataFrame(table)\n",
    "        data.to_csv(r\"C:\\Users\\t1nipun\\Desktop\\PCMR\\human-robot\\Data_Analysis\\csvs\\\\\" + row.tableId + '.csv', encoding = 'utf-8-sig', index = False, header = None)\n",
    "    else:\n",
    "        table[0].extend(['VEC', 'GIS', 'Topic'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}