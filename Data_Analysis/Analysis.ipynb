{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz \n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing environmental variables library that reads from the .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading key-value pairs from the .env file into the OS environment\n",
    "load_dotenv()\n",
    "\n",
    "# Reading the key-value pairs from the OS environment\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASS\")\n",
    "db_hostname = os.getenv(\"DB_HOST\")\n",
    "db_name = os.getenv(\"DB_DATABASE\")\n",
    "\n",
    "# Using those variables in the connection string using \"F\" strings\n",
    "conn_string = f\"mysql+mysqldb://{user}:{password}@{db_hostname}/{db_name}?charset=utf8mb4\"\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "source": [
    "#### Flat Data File for SystemOps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT distinct i.tableId, i.rowIndex, p.pdfName, p.date, p.company, p.monitoring_year, p.monitoring_year_ordinal, pc.consultantName, pr.application_title, i.pipelineName, t.tableTitle, i.land_use_standardized, i.status, i.status_confidence, i.vec_pri, i.vec_sec, w.vec, i.issue_pri, i.issue_sec, s.sub_vec FROM issues i LEFT JOIN tables t ON i.tableId = t.headTable LEFT JOIN pdfs p ON t.pdfName = p.pdfName LEFT JOIN projects pr ON p.application_id = pr.application_id LEFT JOIN pdfsconsultants pc ON p.pdfName = pc.pdfName LEFT JOIN sub_vecs s ON i.tableId = s.tableId AND i.rowIndex = s.rowIndex LEFT JOIN word2vec w ON i.tableId = w.tableId AND i.rowIndex = w.rowIndex WHERE i.status IS NOT NULL;\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    issues_df = pd.read_sql(query, conn)\n",
    "issues_df.fillna('', inplace = True)\n",
    "issues_df = issues_df.astype(str)\n",
    "#issues_df[['monitoring_year', 'monitoring_year_ordinal', 'date', 'status_confidence']] = issues_df[['monitoring_year', 'monitoring_year_ordinal', 'date', 'status_confidence']].astype(str)\n",
    "issues_df = issues_df.groupby(['tableId', 'rowIndex'], as_index = False, sort = False).agg(lambda x: ', '.join(set(x))) # https://stackoverflow.com/questions/50357837/aggregating-string-columns-using-pandas-groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT distinct ip.tableId, ip.rowIndex, ip.rowCounter, p.pdfName, p.date, p.company, p.monitoring_year, p.monitoring_year_ordinal, pc.consultantName, pr.application_title, i.pipelineName, t.tableTitle, i.land_use_standardized, ip.status_issue_parsed, ip.status_confidence_issue_parsed, ip.issue_parsed, w.vec, s.sub_vec FROM pcmr.issues_parsed ip LEFT JOIN sub_vecs s ON ip.tableId = s.tableId AND ip.rowIndex = s.rowIndex AND ip.rowCounter = s.rowCounter LEFT JOIN word2vec w ON ip.tableId = w.tableId AND ip.rowIndex = w.rowIndex AND ip.rowCounter = w.rowCounter LEFT JOIN issues i ON ip.tableId = i.tableId AND ip.rowIndex = i.rowIndex LEFT JOIN tables t ON i.tableId = t.headTable LEFT JOIN pdfs p ON t.pdfName = p.pdfName LEFT JOIN projects pr ON p.application_id = pr.application_id LEFT JOIN pdfsconsultants pc ON p.pdfName = pc.pdfName;\"\n",
    "with engine.connect() as conn:\n",
    "    issues_parsed_df = pd.read_sql(query, conn)\n",
    "issues_parsed_df.fillna('', inplace = True)\n",
    "issues_parsed_df = issues_parsed_df.astype(str)\n",
    "issues_parsed_df = issues_parsed_df.groupby(['tableId', 'rowIndex', 'rowCounter'], as_index = False, sort = False).agg(lambda x: ', '.join(set(x))) # https://stackoverflow.com/questions/50357837/aggregating-string-columns-using-pandas-groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_parsed_df.rename({'status_issue_parsed': 'status', 'status_confidence_issue_parsed': 'status_confidence'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = issues_parsed_df.append(issues_df, ignore_index = True, sort = False)\n",
    "result.to_csv('pcmr_environmental_issues.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "source": [
    "### Rough Work"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9724"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "query = \"SELECT distinct ip.tableId, ip.rowIndex, ip.rowCounter, p.pdfName, p.date, p.company, p.monitoring_year, p.monitoring_year_ordinal, pc.consultantName, pr.application_title, i.pipelineName, t.tableTitle, i.land_use_standardized, ip.status_issue_parsed, ip.status_confidence_issue_parsed, ip.issue_parsed, w.vec, s.sub_vec FROM issues_parsed ip LEFT JOIN issues i ON ip.tableId = i.tableId AND ip.rowIndex = i.rowIndex LEFT JOIN tables t ON i.tableId = t.headTable LEFT JOIN pdfs p ON t.pdfName = p.pdfName LEFT JOIN projects pr ON p.application_id = pr.application_id LEFT JOIN pdfsconsultants pc ON p.pdfName = pc.pdfName LEFT JOIN sub_vecs s ON ip.tableId = s.tableId AND ip.rowIndex = s.rowIndex AND ip.rowCounter = s.rowCounter LEFT JOIN word2vec w ON ip.tableId = w.tableId AND ip.rowIndex = w.rowIndex AND ip.rowCounter = s.rowCounter;\"\n",
    "with engine.connect() as conn:\n",
    "    issues_parsed_df = pd.read_sql(query, conn)\n",
    "issues_parsed_df.fillna('', inplace = True)\n",
    "issues_parsed_df = issues_parsed_df.astype(str)\n",
    "issues_parsed_df = issues_parsed_df.groupby(['tableId', 'rowIndex', 'rowCounter'], as_index = False, sort = False).agg(lambda x: ', '.join(set(x))) # https://stackoverflow.com/questions/50357837/aggregating-string-columns-using-pandas-groupby\n",
    "issues_parsed_df.to_csv('test.csv')\n",
    "len(issues_parsed_df)"
   ]
  },
  {
   "source": [
    "### Data of Issues with no Sub VECs for SystemOps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT distinct i.tableId, i.rowIndex , i.vec_pri, i.vec_sec, i.issue_pri, i.issue_sec, w.word2vec_vec FROM issues i LEFT JOIN issues_parsed ip ON i.tableId = ip.tableId AND i.rowIndex = ip.rowIndex LEFT JOIN sub_vecs s ON i.tableId = s.tableId AND i.rowIndex = s.rowIndex LEFT JOIN word2vec w ON i.tableId = w.tableId AND i.rowIndex = w.rowIndex WHERE s.sub_vec IS NULL AND i.status is not null;\"\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql(query, conn)\n",
    "df['word2vec_vec'].fillna('', inplace = True)\n",
    "aggr_vec_df = df.groupby(['tableId','rowIndex'], as_index = False).agg({'word2vec_vec': ', '.join}) #https://stackoverflow.com/questions/27298178/concatenate-strings-from-several-rows-using-pandas-groupby\n",
    "final_issues_df = pd.merge(aggr_vec_df, df, how = 'left', on = ['tableId', 'rowIndex'])\n",
    "final_issues_df = final_issues_df.drop_duplicates(subset = ['tableId', 'rowIndex'], keep = 'last').reset_index(drop = True)\n",
    "del final_issues_df['word2vec_vec_y']\n",
    "final_issues_df.rename(columns={'word2vec_vec_x':'VECs'}, inplace=True)\n",
    "print(len(final_issues_df))\n",
    "#final_issues_df.to_csv('issues.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT distinct ip.tableId, ip.rowIndex , ip.rowCounter, ip.issue_parsed, w.word2vec_vec FROM issues_parsed ip LEFT JOIN sub_vecs s ON ip.tableId = s.tableId AND ip.rowIndex = s.rowIndex AND ip.rowCounter = s.rowCounter LEFT JOIN word2vec w ON ip.tableId = w.tableId AND ip.rowIndex = w.rowIndex AND ip.rowCounter = w.rowCounter WHERE s.sub_vec IS NULL;\"\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql(query, conn)\n",
    "df['word2vec_vec'].fillna('', inplace = True)\n",
    "aggr_vec_df = df.groupby(['tableId','rowIndex', 'rowCounter'], as_index = False).agg({'word2vec_vec': ', '.join}) #https://stackoverflow.com/questions/27298178/concatenate-strings-from-several-rows-using-pandas-groupby\n",
    "final_issues_df = pd.merge(aggr_vec_df, df, how = 'left', on = ['tableId', 'rowIndex', 'rowCounter'])\n",
    "final_issues_df = final_issues_df.drop_duplicates(subset = ['tableId', 'rowIndex', 'rowCounter'], keep = 'last').reset_index(drop = True)\n",
    "del final_issues_df['word2vec_vec_y']\n",
    "final_issues_df.rename(columns={'word2vec_vec_x':'VECs'}, inplace=True)\n",
    "print(len(final_issues_df))\n",
    "final_issues_df.to_csv('issues_parsed.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "source": [
    "### Creating csv files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                   tableId  rowIndex  \\\n",
       "0     0114daa6-c048-4081-b4cf-7a9d7461fa44         1   \n",
       "1     0114daa6-c048-4081-b4cf-7a9d7461fa44         2   \n",
       "2     0114daa6-c048-4081-b4cf-7a9d7461fa44         3   \n",
       "3     0114daa6-c048-4081-b4cf-7a9d7461fa44         4   \n",
       "4     0114daa6-c048-4081-b4cf-7a9d7461fa44         5   \n",
       "...                                    ...       ...   \n",
       "5185  ffa93da2-0fa1-447a-9930-bb7be8c57ac2         6   \n",
       "5186  ffa93da2-0fa1-447a-9930-bb7be8c57ac2         7   \n",
       "5187  ffa93da2-0fa1-447a-9930-bb7be8c57ac2         8   \n",
       "5188  ffa93da2-0fa1-447a-9930-bb7be8c57ac2         9   \n",
       "5189  ffa93da2-0fa1-447a-9930-bb7be8c57ac2        10   \n",
       "\n",
       "                                                content  \n",
       "0     [[\"Land Use\", \"Condition/ Issues Noted (Rating...  \n",
       "1     [[\"Land Use\", \"Condition/ Issues Noted (Rating...  \n",
       "2     [[\"Land Use\", \"Condition/ Issues Noted (Rating...  \n",
       "3     [[\"Land Use\", \"Condition/ Issues Noted (Rating...  \n",
       "4     [[\"Land Use\", \"Condition/ Issues Noted (Rating...  \n",
       "...                                                 ...  \n",
       "5185  [[\"Parcel Location\", \"March 2012\", \"December 2...  \n",
       "5186  [[\"Parcel Location\", \"March 2012\", \"December 2...  \n",
       "5187  [[\"Parcel Location\", \"March 2012\", \"December 2...  \n",
       "5188  [[\"Parcel Location\", \"March 2012\", \"December 2...  \n",
       "5189  [[\"Parcel Location\", \"March 2012\", \"December 2...  \n",
       "\n",
       "[5190 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tableId</th>\n      <th>rowIndex</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0114daa6-c048-4081-b4cf-7a9d7461fa44</td>\n      <td>1</td>\n      <td>[[\"Land Use\", \"Condition/ Issues Noted (Rating...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0114daa6-c048-4081-b4cf-7a9d7461fa44</td>\n      <td>2</td>\n      <td>[[\"Land Use\", \"Condition/ Issues Noted (Rating...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0114daa6-c048-4081-b4cf-7a9d7461fa44</td>\n      <td>3</td>\n      <td>[[\"Land Use\", \"Condition/ Issues Noted (Rating...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0114daa6-c048-4081-b4cf-7a9d7461fa44</td>\n      <td>4</td>\n      <td>[[\"Land Use\", \"Condition/ Issues Noted (Rating...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0114daa6-c048-4081-b4cf-7a9d7461fa44</td>\n      <td>5</td>\n      <td>[[\"Land Use\", \"Condition/ Issues Noted (Rating...</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>5185</td>\n      <td>ffa93da2-0fa1-447a-9930-bb7be8c57ac2</td>\n      <td>6</td>\n      <td>[[\"Parcel Location\", \"March 2012\", \"December 2...</td>\n    </tr>\n    <tr>\n      <td>5186</td>\n      <td>ffa93da2-0fa1-447a-9930-bb7be8c57ac2</td>\n      <td>7</td>\n      <td>[[\"Parcel Location\", \"March 2012\", \"December 2...</td>\n    </tr>\n    <tr>\n      <td>5187</td>\n      <td>ffa93da2-0fa1-447a-9930-bb7be8c57ac2</td>\n      <td>8</td>\n      <td>[[\"Parcel Location\", \"March 2012\", \"December 2...</td>\n    </tr>\n    <tr>\n      <td>5188</td>\n      <td>ffa93da2-0fa1-447a-9930-bb7be8c57ac2</td>\n      <td>9</td>\n      <td>[[\"Parcel Location\", \"March 2012\", \"December 2...</td>\n    </tr>\n    <tr>\n      <td>5189</td>\n      <td>ffa93da2-0fa1-447a-9930-bb7be8c57ac2</td>\n      <td>10</td>\n      <td>[[\"Parcel Location\", \"March 2012\", \"December 2...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5190 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "source": [
    "query = \"SELECT tableId, rowIndex , content FROM issues;\"\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for row in df.itertuples():\n",
    "    table = json.loads(row.content)\n",
    "    data = pd.DataFrame(table)\n",
    "    data.to_csv(r\"G:\\Post Construction\\csvs\\\\\" + row.tableId + '_' + str(row.rowIndex) + '.csv', encoding = 'utf-8-sig', index = False, header = None)"
   ]
  },
  {
   "source": [
    "### Prepare data for Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT i.tableId, i.rowIndex, ip.rowCounter, i.land_use_standardized, s.sub_vec, i.status, ip.status_issue_parsed, w.vec FROM issues i LEFT JOIN issues_parsed ip ON i.tableId = ip.tableId AND i.rowIndex = ip.rowIndex LEFT JOIN sub_vecs s ON i.tableId = s.tableId AND i.rowIndex = s.rowIndex AND (ip.rowCounter = s.rowCounter OR ip.rowCounter IS NULL) LEFT JOIN word2vec w ON i.tableId = w.tableId AND i.rowIndex = w.rowIndex AND (ip.rowCounter = w.rowCounter OR ip.rowCounter IS NULL);\"\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = ['air', 'fish', 'heritage', 'housekeeping', 'navigation', 'species', 'wildlife', 'water']\n",
    "df['vec'].fillna('', inplace = True)\n",
    "for idx, row in df.iterrows():\n",
    "    for vec in vecs:\n",
    "        if vec in row.vec:\n",
    "            df.at[idx, 'sub_vec'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = df[~(df['vec'].str.contains('vegetation') & ~df['sub_vec'].isin([\"Weed\", \"Vegetation Establishment\", \"Rare Plant\", \"Invasive Plant\"])) & \n",
    "~(df['vec'].str.contains('physical') & ~df['sub_vec'].isin([\"Erosion\", \"Coarse Fragment\", \"Subsidence\"])) & \n",
    "~(df['vec'].str.contains('soil') & ~df['sub_vec'].isin([\"Compaction\", \"Topsoil Loss\"])) &\n",
    "~(df['vec'].str.contains('wetlands') & ~df['sub_vec'].isin([\"Riparian Vegetation Re-establishment\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('issue_parsed_clean.csv', encoding = 'cp1252')\n",
    "print(df1.columns, len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('v_new_parsed_issues.csv', encoding = 'utf-8-sig')\n",
    "df2 = df2.loc[~df2['tableId'].isin([\"3e9e6cdb-f812-4832-b69c-b8ec0396d585\", \"44a33e5f-d99e-48ef-ad56-bbb516ec8796\", \"bfafbfd0-8bb5-4283-8f5e-dd7cbcec480c\"])]\n",
    "print(df2.columns, len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = len(df1) + len(df2)\n",
    "total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('issue_parsed_clean1.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(dataframe, column_name):\n",
    "    dataframe[column_name] = dataframe[column_name].str.lower()\n",
    "    pattern = '|'.join(['<s>', '</s>',])\n",
    "    dataframe[column_name] = dataframe[column_name].str.replace(pattern, '')\n",
    "    dataframe[column_name] = dataframe[column_name].str.replace('\\(s\\)', 's') #reference: https://stackoverflow.com/questions/51440233/how-to-remove-the-values-which-are-in-parentheses-in-pandashttps://stackoverflow.com/questions/51440233/how-to-remove-the-values-which-are-in-parentheses-in-pandas\n",
    "    return dataframe    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM issues WHERE tableId = '3e1c53b4-5c01-46e2-bd72-5a338b5852f9';\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql(query, conn)\n",
    "    #clean_dataframe(df, 'table_content')\n",
    "dic = {}\n",
    "\n",
    "for row in df.itertuples():\n",
    "    # converting JSON string to a list of lists of strings\n",
    "    table = json.loads(row.table_content)\n",
    "    headers = table[0]  # column headers  \n",
    "    for header in headers:\n",
    "        #header = lemmawordnet(header)\n",
    "        #header = clean_header(header)\n",
    "        #header = lemmaspacy(header)\n",
    "        #header = re.sub(r'\\(.*?\\)', lambda x: ''.join(x.group(0).split()), header) # removing whitespace between parentheses (reference: https://stackoverflow.com/questions/34088489/how-to-remove-whitespace-inside-brackethttps://stackoverflow.com/questions/34088489/how-to-remove-whitespace-inside-bracket)\n",
    "        header = \" \".join(header.split())\n",
    "        if header in dic:\n",
    "            dic[header] += 1\n",
    "        else:\n",
    "            dic[header] = 1\n",
    "\n",
    "my_list = [(header, count) for header, count in dic.items()]  # Converting to list\n",
    "my_list.sort(key=lambda tup: tup[1], reverse=True)  # sorting the list\n",
    "\n",
    "total_headers = 0\n",
    "print(f\"COUNT\\tHEADER\")\n",
    "for item in my_list:\n",
    "    total_headers += item[1]\n",
    "    print(f\"{item[1]}\\t{item[0]}\")\n",
    "print()\n",
    "print(f'Total headers: {total_headers}; unique headers: {len(my_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(my_list)\n",
    "df3.to_csv('headers4.csv', encoding = 'utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for row in df.itertuples():\n",
    "    table = json.loads(row.table_content)\n",
    "    print(type(table))\n",
    "    lst_element1 = \"SPREAD\"\n",
    "    lst_element2 = \"QUARTER SECTION\"\n",
    "    if lst_element1 in table[0][0] and lst_element2 in table[0][1]:\n",
    "        print(row.tableId)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}